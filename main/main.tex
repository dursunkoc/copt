\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}


\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage[super]{nth}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage{amssymb,algorithm,algorithmic,algpseudocode}
\usepackage{natbib}
\usepackage{setspace} 
\usepackage{latexsym}
\usepackage{subfig}
\allowdisplaybreaks
\usepackage{array}
\usepackage{subcaption}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\usepackage{pdfpages}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{soul}

\usepackage{capt-of}% or \usepackage{caption}
\usepackage{varwidth}
\newsavebox\tmpbox


\usepackage{geometry}
\geometry{
	a4paper,
	total={160mm,220mm},
	%left=16mm,
	top=22mm,
	bottom=30mm,
}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}



	
\title{Campaign Optimization under Communication Limitations \\}
\author{Dursun KOC $^{a}$ \\ 
	e-mail: dursun.koc@turkcell.com.tr, \\\\
	Dilek Gunnec $^{a}$ \\ 
	e-mail: dilek.gunnec@ozyegin.edu.tr, ORCID ID: 0000-0002-0749-2584 \\\\
$^{a}$ Ozyegin University, Department of Industrial Engineering, Istanbul, Turkey \\ 
$^{\ast}$ Corresponding author \\ }
	
\date{}

\begin{document}
\maketitle
\begin{abstract}
The main objective for marketers is to introduce their companies’ products or services to their customers or potential customers. With this objective they have two different strategies; inbound and outbound marketing strategies. If a marketer wants to introduce a product with an outbound strategy she first selects a target audience, for example people living in a specific city, and age between 20 to 30 namely the young people, then she tries to send her message about the product to them with direct channels such as SMS, outbound call-center or IVR calls. On the other hand, if she wants to introduce her product with an inbound strategy, first she decides rules of whom to interact with. When a customer visits any of the inbound channels for example a web-page or inbound call-center, and the customer fulfills the rules, the message is given to the customer. So inbound marketing is also considered as content marketing. The marketer creates content and tries to gain interests of the customer using social media tactics whereas outbound marketing strategy depends on mass media tools to push the message about the product or the service. Outbound marketing is mostly seen as interruptive and the target audience can easily find a way to dismiss the message. \\

Keywords: Campaign Optimization, Knapsack Problem, Greedy Heuristic, Multi-dimensional Knapsack Problem.

\end{abstract}


\newpage

\section{Introduction}

The main objective for marketers is to introduce their companies’ products or services to their customers or potential customers. With this objective they have two different strategies; inbound and outbound marketing strategies. If a marketer wants to introduce a product with an outbound strategy she first selects a target audience, for example people living in a specific city, and age between 20 to 30 namely the young people, then she tries to send her message about the product to them with direct channels such as SMS, outbound call-center or IVR calls. On the other hand, if she wants to introduce her product with an inbound strategy, first she decides rules of whom to interact with. When a customer visits any of the inbound channels for example a web-page or inbound call-center, and the customer fulfills the rules, the message is given to the customer. So inbound marketing is also considered as content marketing. The marketer creates content and tries to gain interests of the customer using social media tactics whereas outbound marketing strategy depends on mass media tools to push the message about the product or the service. Outbound marketing is mostly seen as interruptive and the target audience can easily find a way to dismiss the message. Besides, from the companies’ perspective it can be very inefficient; because sending a message to a customer who is unlikely to respond is a cost; moreover, not sending the message to a customer who is more likely to respond is a loss from revenue \citep{sarkar}. Deciding who should receive a specific offer is one of the essential questions of outbound (direct) marketing. Marketers have to wisely identify the right target audience because the key to a good relationship with customers is keeping your offers relevant to the customers’ need \citep{malthouse}. An irrelevant communication with customer can cause irritation, and they can block the communication channel, so the company will lose its chance to make a relevant or profitable communication opportunity in the future. Finding the right target audience will not make outbound marketing communication perfect, the most appropriate communication channel and the right time should also be found, and any opportunity to connect to the customers in the future should not be lost.\\

Outbound marketing (also called targeted advertising) needs large amount of personal data of customers, for being effective. So companies are collecting more information about their customers, which causes an information asymmetry \citep{waerdt}. As the governments are responsible for protecting their citizens’ rights, they make laws and regulations on this information asymmetry. The General Data Protection Regulation (GDPR) law in EU is an example. According to such regulations; a customer can request from the company to stop processing its data. Therefore, companies need to apply some limitation on outbound communication to their companies.\\

Outbound marketing campaigns need to be very well planned and optimized with regard to time, target audience, channel and content. In this study we will optimize outbound campaigns regarding time and channel with keeping communication under certain limitations, those limitations can either be a result of legislation or a result of marketing strategy not to irritate customer.\\

The remaining of this paper is structured as follows. In \S \ref{s:literature-review}, we review the related literature. In \S \ref{s:problem-model}, we describe the problem and present our mathematical model. In \S \ref{s:solution-method}, we describe our solution methods. In \S \ref{num-analysis}, we present computational results. Finally, we present our conclusions in \S \ref{s:conclusion}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Literature Review}  \label{s:literature-review}

In the industry, outbound campaigns are mostly designed regarding the target audience. Machine learning methods, statistics and business rules and regulations are used to find the right target audience, and the message about the product is sent to customers; moreover database marketing community also tries to estimate the expected value of an offer to a specific customer from a specific channel \citep{cohen_exp, oliveira_hypr}. In direct marketing, marketers select a portion of the customer group with the highest probability of expecting the offer, and then add them to their target audience for the marketing campaign \citep{owczarczuk}. However, even if the offer addresses the customer’s need, most of the time customer can dismiss the message because of the wrong channel or because of the wrong time. Having the right channel and the right time with the right audience is the most desired case, but it is not free. The cost of some channels can be high while some others are cheaper. Further, with social network effects some of the targeting can become unnecessary and costly.\\

In today’s world, marketers have multiple channels to introduce their product or services, and many studies have shown the importance of omni-channel sales strategies \citep{shankar, park}. In the essence customers want to interact with the company via multiple channels, companies’ challenge is to provide a nearly identical environment for each channel \citep{bell}. Customer can get information about a product or service from one channel, but the purchase can be from another channel \citep{park}. Even the information about the product or service can be solidified by a series of messages from different channels. At this point it is crucial to employ additional constraints regarding multiple channels when optimizing the campaign.\\

Most of the studies about campaign optimization focus on obtaining the right target audience for the campaign \citep{goul}; however, they usually ignore campaign channels, or optimize the online campaigns. They are finding the best matched campaign for a given online search only, or web-page banner optimization \citep{liu}, and the success criteria or the objective function for such models is often the click through rate or the action rate. Such objective functions do not truly represent the advertisers’ benefit and rather focus on the publishers’ benefit \citep{altshuler}. Our model will be directly related to the advertisers’ benefit.\\

Different studies work on different industries and various business constraints; but the mathematical models mostly seems similar, and can be counted as assignment problem, or more specifically multi-dimensional knapsack problem \citep{cohen_exp, oliveira_hypr}. For solving ILP (Integer Linear Programming), MIP(Mixed Integer Programming) models, numerous methods and tools have been introduced\citep{fallah_bb, chu_mip}, however due to the complexity of the problem, handling big models remains a technical difficulty. Although the branch and bound (B&B) algorithm has been presented as the definitive and deterministic solution, when the number of variables and the number of constraints get larger, the B&B algorithm does not provide a solution in reasonable time \citep{herrera_pbb, sato}. Because of that reason some work on parallel B&B algorithm\citep{fallah_bb, sato}; but computational resources required to run the algorithm increases.\\ \citeauthor{cohen_exp} applied a greedy approach to a similar problem \citep{cohen_exp}. Multidimensional knapsack problems having multiple resource constraints with binary decision variables, which is similar to our model have been dealt with greedy-like heuristic methods \citep{akcay_mdkp}.

Although studies related to campaign management are common in marketing, there is increasing interest to this area from operations research and computer science areas, as the abundant data require complex data analysis skills. In my thesis, after such data analysis, we plan to introduce optimization methods which are not yet commonly used to tackle such problems. Specifically, one of our main contributions will be utilizing from network optimization tools to produce high quality solutions.\\

In this study, we plan to develop a model which tries to maximize the number of interactions for the campaigns decided by marketing team, while not exceeding the communication limitations. In the second place, we also try to increase the number of interaction to the most influential customers. We believe that influential customers can spread our message to a broader audience. In order to find the influential customer we will use customers' call and product purchase data from Turkcell.

\section{Problem Definition and Mathematical Model}  \label{s:problem-model}

In this section, we describe the proposed mathematical model to maximize the number of interactions to customer by adhering the communication limitations. In \S \ref{s:problem-desc}, we detail the limitations to customer communication and in \S \ref{s:problem-math}, we present our mathematical model.

\subsection{Problem Description} \label{s:problem-desc}

Turkcell, a telecommunication company in Turkey, provides different tariffs, services and product to its customers. In order to introduce these services and products to its customers and potential customers, the marketing team launches daily and weekly basis campaigns. Before launching these campaigns, they do some data analysis to find the right targeting group. After finding the right target audience, they apply a set of business rules to optimize their communication with their customers. During this optimization process, filters are applied to the target audience in order to keep the communication to customers in certain levels. The first filter assure that no customers are targeted by the same campaign from different channels. Next, they limit the number of total interactions to a customer in one day. Campaigns are categorized in two dimensions; first dimension is quota categories which describes filters for a campaign and the second one is priority categories which describes the importance of a campaign. Each campaign belongs to a quota category and each quota category has a limitation on the number of messages sent to a customer, both in a daily, and a weekly basis; moreover each campaign has a limitation unique to its own; and finally, each communication channel has a capacity that should not be exceeded for each day. Each campaign is bound to a priority category.\\

Turkcell launches about 70 campaigns on average for around 60-70 million customers through 3 channels each day, and these campaigns are planned in a weekly basis. Number of quota categories is fixed to 3, but priority categories can be variable around 10.\\

In our study to optimize the Turkcell’s outbound campaign management we start with an initial model that accepts the target audience without any inference, but in the next steps we will shrink the target audience using network optimization technics. The motivation behind using network optimization is the idea that people communicating can carry our message.

\subsection{Mathematical Model} \label{s:problem-math}

In this section, we present a mixed integer programming model for the campaign optimization problem described at \S \ref{s:problem-desc}. We first introduce the notation and then present the mathematical model.\\

% General Math Model
\noindent \textbf{Sets}\\

\noindent ${\mathcal{C}}$: set of campaigns. \\
\noindent ${\mathcal{U}}$: set of customers. \\
\noindent ${\mathcal{H}}$: set of channels. \\
\noindent ${\mathcal{D}}$: set of planning days. \\
\noindent ${\mathcal{I}}$: set of quota categories. \\
\noindent ${\mathcal{P}}$: set of priority categories. \\


\noindent \textbf{Decision Variables}\\

\noindent $X_{{c}{u}{h}{d}}=1$, if campaign $c \in \mathcal{C}$ will be sent to customer $u \in \mathcal{U}$ through channel $h \in \mathcal{H}$ at day $d \in \mathcal{D}$, and 0 otherwise.
($\forall c \in \mathcal{C}$, $\forall u \in \mathcal{U}$, $\forall h \in \mathcal{H}$, $\forall d \in \mathcal{D}$ )\\

\noindent \textbf{Parameters}\\

\noindent $e_{{c}{u}}=1$, if customer $u \in \mathcal{U}$ is eligible for campaign $c \in \mathcal{C}$, and 0 otherwise.
($\forall c \in \mathcal{C}$, $\forall u \in \mathcal{U}$)\\

\noindent $s_{{c}{u}{h}{d}}=1$, if customer $u \in \mathcal{U}$ received a communication for campaign $c \in \mathcal{C}$, through channel $h \in \mathcal{H}$, at day $d \in \mathcal{D}$ previous week and 0 otherwise.
($\forall c \in \mathcal{C}$, $\forall u \in \mathcal{U}$, $\forall h \in \mathcal{H}$, $\forall d \in \mathcal{D}$ )\\

\noindent $q_{{i}{c}}=1$, if campaign $c \in \mathcal{C}$ is in $i \in \mathcal{I}$ quota category, and 0 otherwise.
($\forall c \in \mathcal{C}$, $\forall i \in \mathcal{I}$)\\

\noindent $r_{{c}{p}}$ priority value of campaign $c \in \mathcal{C}$ regarding priority type $p \in \mathcal{P}$.
($\forall c \in \mathcal{C}$, $\forall p \in \mathcal{P}$)\\

\noindent $b$ communication limit per customer $u \in \mathcal{U}$ for the whole period.
($\forall u \in \mathcal{U}$)\\

\noindent $k$ communication limit per customer $u \in \mathcal{U}$ at each day.
($\forall u \in \mathcal{U}$)\\

\noindent $l_{c}$ communication limit per customer $u \in \mathcal{U}$ for campaign $c \in \mathcal{C}$.
($\forall c \in \mathcal{C}$, $\forall u \in \mathcal{U}$)\\

\noindent $m_{i}$ communication limit per customer $u \in \mathcal{U}$ for $i \in \mathcal{I}$ quota category for the whole period.
($\forall u \in \mathcal{U}$, $\forall i \in \mathcal{I}$)\\

\noindent $n_{i}$ communication limit per customer $u \in \mathcal{U}$ for $i \in \mathcal{I}$ quota category at each day.
($\forall d \in \mathcal{D}$, $\forall u \in \mathcal{U}$, $\forall i \in \mathcal{I}$)\\

\noindent $t_{{h}{d}}$ capacity for channel $h \in \mathcal{H}$ at day $d \in \mathcal{D}$.
($\forall h \in \mathcal{H}$, $\forall d \in \mathcal{D}$)\\

\noindent The formulation for the campaign optimization is presented next.

\begin{align}
\text{Maximize} & \displaystyle
\sum\limits_{p\in \mathcal{P}}
\sum\limits_{c\in \mathcal{C}}
\sum\limits_{u\in \mathcal{U}}
\sum\limits_{h\in \mathcal{H}}
\sum\limits_{d\in \mathcal{D}}
X_{{c}{u}{h}{d}} \cdot r_{{c}{p}} \label{mathmodel_obj}&\\
\text{subject to} \notag\\
&X_{{c}{u}{h}{d}} \leq e_{{c}{u}},&\forall c \in \mathcal{C}, \forall u \in \mathcal{U}, \forall h \in \mathcal{H}, \forall d \in \mathcal{D} \label{mathmodel_eligibility}&\\
&\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} \leq 1, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U}, \forall d \in \mathcal{D} \label{mathmodel_singlechannel}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in\mathcal{D}}X_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [2,7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh1}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1,2]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [3,7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh2}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1,3]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [4,7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh3}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1,4]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [5,7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh4}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1,5]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [6,7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh5}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [1,6]}X_{{c}{u}{h}{d}} + \sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}\sum\limits_{d\in [7]}s_{{c}{u}{h}{d}} \leq b, &\forall u \in \mathcal{U} \label{mathmodel_percustomercommlimit_rh6}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \leq k, &\forall u \in \mathcal{U}, \forall d \in \mathcal{D} \label{mathmodel_percustomercommlimit_day}&\\
&\sum\limits_{d\in\mathcal{D}}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit}&\\
&\sum\limits_{d\in[1]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[2,7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh1}&\\
&\sum\limits_{d\in[1,2]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[3,7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh2}&\\
&\sum\limits_{d\in[1,3]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[4,7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh3}&\\
&\sum\limits_{d\in[1,4]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[5,7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh4}&\\
&\sum\limits_{d\in[1,5]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[6,7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh5}&\\
&\sum\limits_{d\in[1,6]}\sum\limits_{h\in\mathcal{H}}X_{{c}{u}{h}{d}} + \sum\limits_{d\in[7]}\sum\limits_{h\in\mathcal{H}}s_{{c}{u}{h}{d}} \leq l_{c}, &\forall c \in \mathcal{C}, \forall u \in \mathcal{U} \label{mathmodel_percustomercamplimit_rh6}&\\
&\sum\limits_{d\in\mathcal{D}}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit}&\\
&\sum\limits_{d\in[1]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[2,7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh1}&\\
&\sum\limits_{d\in[1,2]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[3,7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh2}&\\
&\sum\limits_{d\in[1,3]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[4,7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh3}&\\
&\sum\limits_{d\in[1,4]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[5,7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh4}&\\
&\sum\limits_{d\in[1,5]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[6,7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh5}&\\
&\sum\limits_{d\in[1,6]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} + \sum\limits_{d\in[7]}\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}s_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq m_{i}, &\forall u \in \mathcal{U}, \forall i \in \mathcal{I} \label{mathmodel_weeklyquotalimit_rh6}&\\
&\sum\limits_{h\in\mathcal{H}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \cdot q_{{i}{c}} \leq n_{i}, &\forall u \in \mathcal{U}, \forall d \in \mathcal{D}, \forall i \in \mathcal{I} \label{mathmodel_dailyquotalimit}&\\
&\sum\limits_{u\in\mathcal{U}}\sum\limits_{c\in\mathcal{C}}X_{{c}{u}{h}{d}} \leq t_{{h}{d}}, &\forall d \in \mathcal{D}, \forall h \in \mathcal{H} \label{mathmodel_channellimit}&\\
&X_{{c}{u}{h}{d}} \in \{0,1\},&\forall c \in \mathcal{C}, \forall u \in \mathcal{U}, \forall h \in \mathcal{H}, \forall d \in \mathcal{D} \label{mathmodel_integrity}
\end{align}\\

The objective function \eqref{mathmodel_obj} maximizes campaign communication regarding the campaign priority. Constraints \eqref{mathmodel_eligibility} ensure that the communication will be placed when the customer is eligible for the campaign. Constraints \eqref{mathmodel_singlechannel} say that a customer should not be targeted for the same campaign from different channels. Constraints \eqref{mathmodel_percustomercommlimit} defines an upper-bound for total number of communication to each customer for the whole period; and constraints from \eqref{mathmodel_percustomercommlimit_rh1} to \eqref{mathmodel_percustomercommlimit_rh6} extends the upper-bound for total number of communication to each customer regarding the previous period. Constraints \eqref{mathmodel_percustomercommlimit_day} defines an upper-bound for total number of communication to each customer for a single day. Constraints \eqref{mathmodel_percustomercamplimit} defines an upper-bound for total number of communication to each customer per campaign for the whole period; and constraints from \eqref{mathmodel_percustomercamplimit_rh1} to \eqref{mathmodel_percustomercamplimit_rh6} extends the upper-bound for total number of communication to each customer per campaign regarding the previous period. Campaigns are group by their marketing purpose, and for each of these groups we have combined limitations, constraints \eqref{mathmodel_weeklyquotalimit} draws a limitation on the number of communications about campaigns that fell in specific groups. $q_{{i}{c}}$ stands for if the campaign $c$ is in the $i$ category. Likewise constraints from \eqref{mathmodel_weeklyquotalimit_rh1} to \eqref{mathmodel_weeklyquotalimit_rh6} limits the number of communications for campaign quota categories regarding previous period. Constraints \eqref{mathmodel_dailyquotalimit} ensures that the number of communications for campaign quota categories for each day is not exceeded. Constraint \eqref{mathmodel_channellimit} ensures that each communication channels' capacity are not exceeded; and finally constraint \eqref{mathmodel_integrity} ensures that our variable $X_{{c}{u}{h}{d}}$ is either 0 or 1.\\

Due to its complexity, large instances of the campaign optimization problem may not be solved quickly by using exact methods. To support time to market objectives of marketing teams, we developed a heuristic approach that can attain high-quality solutions within a reasonable duration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solution Method: A Greedy Heuristic}  \label{s:solution-method}

In this section, we describe a Greedy based heuristic developed to solve the campaign optimization problem described in \ref{s:problem-desc}. In general a greedy algorithm heuristic is based on the selection of the best available choice. In a classical knapsack problem, where we need to fill a bag with some stuff; and each stuff has a weight $w$ and a value $v$. We need to decide on which items to be put on the bag, or shall we put the $i^{th}$ item on the bag or not $X_{i}$, by not exceeding the capacity $C$ and gaining the maximum value. The mathematical formulation can expressed as following;
\begin{align*}
\text{Maximize} & \displaystyle
\sum\limits_{i\in \mathcal{I}} X_{i} \cdot v_{i} \label{knapsack_mathmodel_obj}&\\
\text{subject to} \notag\\
&\sum\limits_{i\in \mathcal{I}}X_{i} \cdot w_{i} \leq C, \label{knapsack_mathmodel_capacity}&\\
&X_{i} \in \{0,1\},&\forall i \in \mathcal{I} \label{knapsack_mathmodel_integrity}
\end{align*}\\

Here the greedy approach tells us to calculate the unit value $v_{i}/w_{i}$ of each item, and sort items by their calculated unit value, starting from the greatest unit value to smallest try to add to bag until the last item, by not exceeding the capacity.


\begin{algorithm}[H]
\KwIn{A set $w = \{w_1, w_2, \ldots, w_n\}$ weight of items}
\KwIn{A set $v = \{v_1, v_2, \ldots, v_n\}$ value of items}
\KwOut{A list of $X_1,X_2,\ldots,X_k$, such that $\sum_{i=1}^n X_i \cdot w_i \leq C$ and ${\sum\limits_{i\in \mathcal{I}} X_{i} \cdot v_{i}}$ is maximized}
$b \gets 0$\;

\For{$i \gets 1$ \textbf{to} $n$}{
  $X_i \gets 0$\;
\\
  \If{$b + w_i \leq C$} {
    $X_i \gets 1$\;
\\
    $b \gets $b + w_i\;
  }
}
\Return{$X$}\;
\caption{Greedy Algorithm for a classical Knapsack Problem}
\label{algo:change}
\end{algorithm}\\

We use the following greedy algorithm to solve the campaign optimization problem described in \ref{s:problem-desc}.

\begin{algorithm}[H]
\KwIn{A set $c\in\mathcal{C}$ of campaigns}
\KwIn{A set $u\in\mathcal{U}$ of customers}
\KwIn{A set $h\in\mathcal{H}$ of channels}
\KwIn{A set $d\in\mathcal{D}$ of days}
\KwOut{A list of $X_{{c}{u}{h}{d}}$, such that equations from \eqref{mathmodel_eligibility} to \eqref{mathmodel_integrity} are satisfied}
\\
Order and reindex campaigns $c\in\mathcal{C}$ so that $r_{c_{1}p} \geq r_{c_{2}p}$
\\
\For{$c \gets 1$ \textbf{to} $C$}{
    \For{$d \gets 1$ \textbf{to} $D$}{
        \For{$h \gets 1$ \textbf{to} $H$}{
            \For{$u \gets 1$ \textbf{to} $U$}{
                 $X_{cuhd} \gets 1$\;
\\
                $EQ$ \gets Equations containing $X_{cuhd}$ 
\\
                 \If{not $EQ$ satisfied} {
                    $X_{cuhd} \gets 0$\;
                 }
            }
        }
    }
}
\Return{$X$}\;
\caption{Greedy Algorithm for Campaign Optimization}
\label{algo:change}
\end{algorithm}\\


\section{Computational Study} \label{num-analysis}

In this section, we present the results of our computational study conducted to evaluate the performance of our greedy heuristic algorithms. We describe our test instances in \S \ref{test_instances}. We evaluate the performance of the greedy heuristics in \S \ref{test_evaluation}.

All computations were performed on a computer with 64-bit Windows 10 operating system with Intel(R) Core(TM) i7-3630QM CPU 16 GB RAM, and CPLEX 20.1 was used in Python to solve campaign optimization for exact results. Greedy heuristic algorithm is also coded in Python using Numpy library for fast mathematical operations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Test Instances} \label{test_instances}

%\begin{itemize}
%\item \textit{Network structure}. We generate small and large sized instances based on networks with 40 and 80 nodes, respectively. In Tables 3 and 4, $|\mathcal{N}|$ is the total number of infrastructure nodes, while $|N_p|$ and $|N_g|$ show the number of nodes in the power and gas networks, respectively. To generate a network with $|\mathcal{N}|$ nodes, we obtain the first $|\mathcal{N}|$ nodes of  100-node random Solomon instances \citep{solomon1987algorithms}, and assign the first $|N_p|$ of these $|\mathcal{N}|$ nodes as the power infrastructure nodes. The sites with damaged nodes in both networks are chosen randomly. The repair centers of the power and gas networks, $D_p$ and $D_g$, are located at coordinates (45,50) and (50,45), respectively. Network travel times are set equal to the Euclidean distances.

%Given the set of nodes, we generate instances that differ in terms of the locations of damaged nodes, interdependency links, and repair times. As shown in Table 3, we consider two sets of small instances. Locations of damaged nodes and interdependency links are the same for the instances with the same number of damaged nodes in the same set. In all small instances, we consider two repair teams (vehicles) per infrastructure (i.e., $k_{p}$=$k_{g}$=2). For large instances, we create six different network structures, and solve instances with two and three teams for each network (Table 4).

%They are generated from Solomon instances (\ref{}) present in the literature. In small sized data, power (gas) nodes of Set 1 and Set 2 instances are chosen as the first (second) 20 nodes of one Solomon instance. Similarly in large sized data, power (gas) nodes of Set 1 and Set 2 instances are chosen as the first (second) 40 nodes of one Solomon instance. {\color{red}Set 2?}  


%\item \textit{Interdependencies}. For each instance, the number of interdependency links that exist within and between different networks, denoted by $\delta_{ij} \: \forall i,j \in \mathcal{N}$, are presented in Tables 3 and 4. For example, $\delta_{pg}$ is the total number of links that represents a dependency of power network nodes on gas network nodes. The interdependency links are generated by selecting pairs of interdependent nodes randomly. Note that if dependencies are created only among the undamaged nodes, we may not capture the potential difficulties caused by interdependencies among the damaged nodes. Therefore, in generating the test instances, we use the Instance Generator to make sure that at least $50\%$ of the interdependency links are among the damaged nodes. 


\begin{table}[t!]
    \footnotesize
    \centering
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
Instance & Set & $|\mathcal{N}|$ & $|N_p|$ & $|N_g|$ & $|B_p|$ & $|B_g|$ & $r_i$ & $\delta_{pp}$ & $\delta_{gg}$ & $\delta_{pg}$ & $\delta_{gp}$ \\ \midrule \hline
1 & 1 & 40 & 20 & 20 & 12 & 12 & 40 & 4 & 4 & 2 & 2 \\
2 & 1 & 40 & 20 & 20 & 12 & 12 & 40 & 6 & 6 & 3 & 3 \\
3 & 1 & 40 & 20 & 20 & 12 & 12 & 40 & 8 & 8 & 4 & 4 \\
4 & 1 & 40 & 20 & 20 & 12 & 12 & 40 & 12 & 12 & 5 & 5 \\\hline
5 & 1 & 40 & 20 & 20 & 8 & 8 & 40 & 4 & 4 & 2 & 2 \\
6 & 1 & 40 & 20 & 20 & 8 & 8 & 40 & 6 & 6 & 3 & 3 \\
7 & 1 & 40 & 20 & 20 & 8 & 8 & 40 & 8 & 8 & 4 & 4 \\
8 & 1 & 40 & 20 & 20 & 8 & 8 & 40 & 12 & 12 & 5 & 5 \\\hline
9 & 1 & 40 & 20 & 20 & 12 & 12 & 30-50 & 4 & 4 & 2 & 2 \\
10 & 1 & 40 & 20 & 20 & 12 & 12 & 30-50 & 6 & 6 & 3 & 3 \\
11 & 1 & 40 & 20 & 20 & 12 & 12 & 30-50 & 8 & 8 & 4 & 4 \\
12 & 1 & 40 & 20 & 20 & 12 & 12 & 30-50 & 12 & 12 & 5 & 5 \\\hline
13 & 1 & 40 & 20 & 20 & 8 & 8 & 30-50 & 4 & 4 & 2 & 2 \\
14 & 1 & 40 & 20 & 20 & 8 & 8 & 30-50 & 6 & 6 & 3 & 3 \\
15 & 1 & 40 & 20 & 20 & 8 & 8 & 30-50 & 8 & 8 & 4 & 4 \\
16 & 1 & 40 & 20 & 20 & 8 & 8 & 30-50 & 12 & 12 & 5 & 5 \\\hline\hline
17 & 2 & 40 & 20 & 20 & 12 & 12 & 40 & 4 & 4 & 2 & 2 \\
18 & 2 & 40 & 20 & 20 & 12 & 12 & 40 & 6 & 6 & 3 & 3 \\
19 & 2 & 40 & 20 & 20 & 12 & 12 & 40 & 8 & 8 & 4 & 4 \\
20 & 2 & 40 & 20 & 20 & 12 & 12 & 40 & 12 & 12 & 5 & 5 \\\hline
21 & 2 & 40 & 20 & 20 & 8 & 8 & 40 & 4 & 4 & 2 & 2 \\
22 & 2 & 40 & 20 & 20 & 8 & 8 & 40 & 6 & 6 & 3 & 3 \\
23 & 2 & 40 & 20 & 20 & 8 & 8 & 40 & 8 & 8 & 4 & 4 \\
24 & 2 & 40 & 20 & 20 & 8 & 8 & 40 & 12 & 12 & 5 & 5 \\\hline
25 & 2 & 40 & 20 & 20 & 12 & 12 & 30-50 & 4 & 4 & 2 & 2 \\
26 & 2 & 40 & 20 & 20 & 12 & 12 & 30-50 & 6 & 6 & 3 & 3 \\
27 & 2 & 40 & 20 & 20 & 12 & 12 & 30-50 & 8 & 8 & 4 & 4 \\
28 & 2 & 40 & 20 & 20 & 12 & 12 & 30-50 & 12 & 12 & 5 & 5 \\\hline
29 & 2 & 40 & 20 & 20 & 8 & 8 & 30-50 & 4 & 4 & 2 & 2 \\
30 & 2 & 40 & 20 & 20 & 8 & 8 & 30-50 & 6 & 6 & 3 & 3 \\
31 & 2 & 40 & 20 & 20 & 8 & 8 & 30-50 & 8 & 8 & 4 & 4 \\
32 & 2 & 40 & 20 & 20 & 8 & 8 & 30-50 & 12 & 12 & 5 & 5 \\ \bottomrule
\end{tabular}
\caption{Characteristics of small instances.}
	\label{instance_information}% 
\end{table}


\begin{table}[h!]
    \footnotesize
    \centering
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
Instance & $k_p$=$k_g$ & $|\mathcal{N}|$ & $|N_p|$ & $|N_g|$ & $|B_p|$ & $|B_g|$ & $r_i$ & $\delta_{pp}$ & $\delta_{gg}$ & $\delta_{pg}$ & $\delta_{gp}$ \\ \midrule
33 & 2 & 80 & 40 & 40 & 20 & 20 & 30-50 & 8 & 8 & 4 & 4 \\
34 & 2 & 80 & 40 & 40 & 20 & 20 & 30-50 & 12 & 12 & 6 & 6 \\
35 & 2 & 80 & 40 & 40 & 20 & 20 & 30-50 & 18 & 18 & 9 & 9 \\
36 & 2 & 80 & 40 & 40 & 25 & 25 & 30-50 & 8 & 8 & 4 & 4 \\
37 & 2 & 80 & 40 & 40 & 25 & 25 & 30-50 & 12 & 12 & 6 & 6 \\
38 & 2 & 80 & 40 & 40 & 25 & 25 & 30-50 & 18 & 18 & 9 & 9 \\\hline
39 & 3 & 80 & 40 & 40 & 20 & 20 & 30-50 & 8 & 8 & 4 & 4 \\
40 & 3 & 80 & 40 & 40 & 20 & 20 & 30-50 & 12 & 12 & 6 & 6 \\
41 & 3 & 80 & 40 & 40 & 20 & 20 & 30-50 & 18 & 18 & 9 & 9 \\
42 & 3 & 80 & 40 & 40 & 25 & 25 & 30-50 & 8 & 8 & 4 & 4 \\
43 & 3 & 80 & 40 & 40 & 25 & 25 & 30-50 & 12 & 12 & 6 & 6 \\
44 & 3 & 80 & 40 & 40 & 25 & 25 & 30-50 & 18 & 18 & 9 & 9 \\\bottomrule
\end{tabular}
\caption{Characteristics of large instances.}
	\label{_largeinstance_information}% 
\end{table}


%There are two types of dependency relationships: (i) within the nodes of an infrastructure and (ii) between the nodes of infrastructures. The number of dependent nodes are equally distributed between two infrastructures. When all the dependent nodes depend on undamaged nodes, we get a relatively easy instance, i.e., there is no node to be repaired. To avoid this, we determine the dependencies so that 

%To construct dependency relationships, we select how many dependencies of a particular infrastructure has (both from the same infrastructure and the other one). {\color{red} how many nodes of a particular infrastructure will be dependent on how many other nodes (both from the same infrastructure and the other one) Sadece sayilari belirliyoruz degil mi, 3 power 5 gas'a bagli olsun gibi?} We make sure ({\color{red} how?}) that we select the type of component such as damaged or nondamaged, and also we create an option for random selection so that dependency links include both damaged and undamaged nodes. Therefore the dependency structure in the problem is not lost. 
%\vspace{-0.7cm}

%\item \textit{Repair times}. Repair times, $r_i$ $\forall i \in \mathcal{B}$, used in our instances are presented in Tables 3 and 4. We consider instances with equal repair times, which are set at 40 time units for each damaged node, and instances with different repair times, in which repair times are generated randomly between 30 and 50.
%\end{itemize}

%These values are set by considering the average network travel time between all nodes, which is 33.4.

%To observe the effect of the repair time on the problem, the parameter settings for the first and the last 8 instances in Set 1 and 2 of small sized data are kept the same, i.e., the only difference between these instances are the repair time generation method, including the same damaged node locations and dependency relationships.



%Small instances have the same set of nodes (i.e., same coordinates), however half of the instances have a different dependency structure. Thus, we have two sets (Set 1 and 2) of instances with which represent two different dependency structures on the same network. The instances within the same set has the same parameter settings, as well. Note that, although the parameter values are set the same, instances are different (e.g., 8 damaged nodes in both Set 1 and Set 2, but different randomly selected 8 nodes in each instance {\color{red}bu yazdigim dogru mu? Dogru}). In addition, there are two sets of six instances in large sized data. Large instances have exactly the same characteristics, except that they differ in the number of vehicles. In the first set of instances, there are two vehicles in each infrastructure whereas the second set have three vehicles so that we also see the effect of creating a change in the number of vehicles on the problem.


%Instance on networks with 40 nodes in total. We create two instance sets, where they have the same parameter settings for all but dependency assignments. Therefore, we create two sets of test instances that have 16 instances, 32 in total, in order to observe if and how randomness affects the problem. Besides the difference resulting from randomness, these two sets of instances have some mutual characteristics. 

%To be more precisely, Solomon instance is benefitted as creating  all test instances. As it is mentioned earlier, for each instance, we have 40 nodes in total. First 20 nodes in one set have chosen as the first 20 nodes (\nth{1}-\nth{20}) in Solomon and used as “power” nodes. Also,  second set with 20 nodes have chosen as the second 20 nodes in the Solomon dataset (\nth{21}-\nth{40}) but used as “gas” nodes. In addition to the same 40 nodes and their locations, all instances in two sets have 2 repair centers, one gas and one power. 

%We create two sets although they have many mutual features in design. The aim is to give different values for the number of damaged/non-damaged nodes, repair times of nodes and their dependencies. We purpose to observe the impact of randomness by giving not only different values for these factors but also assigning these values to different locations. To be more specific, two instances may have exactly same numbers for all nodes and random factors but they are completely different test instances since the numbers are assigned to differently and randomly located nodes. 

%\textit{Node functionality:} All nodes are either damaged or undamaged. (Note that, an undamaged node may be non-functional if it depends on a damaged node.) In each instance for small sized data, we set a partitioning of 12 and 8 nodes. That means, there are either 12 or 8 randomly selected damaged nodes in each infrastructure in any instance. In all large instances, there are randomly selected 20 damaged nodes out of 40. 

 

%The longest and shortest distance between two nodes are measured among 20 nodes for each infrastructure, as well as the average. The longest distance of time is calculated as 88.8, shortest is 2.2 and average is 33.4. As identifying 40 and 30-50 range for two types of repair times, we take into account that this value should not be more than 88.8 and less than 2.2. Additionally, the crucial point here is that we observe the effect of changing repair times on the problem independently from every others. The instances have the same numbers for the status and dependencies of nodes but different repair times are created. The aim is monitoring especially the effect of repair times. For example, \nth{1} instance in first set is structured as the same with the \nth{9} instance in the second set. Their mere difference is the repair times. Here, they have not only the same values for other factors but also the same node locations and dependency conditions. Hence, everything kept the same except repair times for them so that how this change affects the infrastructure network is analyzed. 



%The first type is when the dependency is within the infrastructure itself, meaning the same network (intra-network dependency). For instance, a gas node may be dependent on another gas node and vice versa. The second type is when the dependency is among different infrastructures (inter-network dependency). To exemplify, a gas node may be dependent on a power node.

%The numbers assigned to dependencies are carefully identified based on their damage status. We set a condition that the $50\%$ of the given number has to belong to damaged nodes. The possibility that dependencies are among only undamaged nodes and the case that their dependencies have no meaning have been eliminated.

%In total, we carry out our computational experiments with 32 test instances, first 16 belong to first set, the rest is to second. The characteristics of the instances are listed in the below instance information tables (Table \ref{instance_information}).

%In the table, instance numbers, information on the network sizes, number of broken nodes on each network and dependency types are given in the first seven columns. Specifically, $|N|$ refers to the total number of nodes except the repair center nodes. $|N_p|$ and $|N_g|$ refer to the number of nodes in the power and gas networks, respectively. The number of broken nodes in power and gas networks are represented by $|B_p|$ and $|B_g|$. Additionally, repair time for power and gas nodes are denoted by $RT_p$ and $RT_g$, respectively. Finally, intra and inter network dependencies are presented with $|D|$ based on their dependency conditions.

%\textit{Number of power and gas nodes:} Number of power (gas) nodes are selected as $\%50$ $(\%50)$, $\%60$ $(\%40)$ and $\%75$ $(\%25)$ of the total number of nodes.  Furthermore, we assume that $\%20$ and $\%60$ of the number of nodes in each infrastructure are broken nodes. The broken nodes are set randomly.

%(See Tables  \ref{20nodes_details}, \ref{50nodes_details} and \ref{100nodes_details} in the Appendix).
	
%\textit{Interdependency:} Two interdependency types are considered for each instance. The first type is when only gas nodes may be dependent on power nodes; i.e., unidirectional interdependency (Type 1). The second one is when both network nodes may be dependent on each other; i.e., bidirectional interdependency (Type 2). 

%In all instances, two vehicles are assigned to each infrastructure network. The repair time of all nodes are assumed to be one hour. In total, we carry out our computational experiments with 48 test instances. The characteristics of the instances are listed in the result tables (Table \ref{comparecplex20} and \ref{50nodes-bounds}).

%An example of a 20 node network may be found in Figure \ref{Third_app}. 
%For details on instances, you can see in Tables \ref{20nodes_details}, \ref{50nodes_details} and \ref{100nodes_details} in the Appendix. 
%SRPRIIN solutions and heuristics performances are studied in section \ref{RIISRP_Solutions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% instance_information


\subsection{Implementation: SA Parameters and Variants}

Our SA heuristic requires a few parameters to be set in advance, as explained in \S 4.3. We consider a geometric cooling schedule, and set the cooling rate as 0.90 based on the results of our experiments applied with different cooling rates (Appendix C). The epoch length is set as two; that is, two iterations are performed at the same temperature value. The SA algorithm terminates when the final temperature, $T_{f}$, reaches 0.001 or the one-hour computational time limit is reached. We followed the approach in \cite{ropke_adaptive_2006} and \cite{wei_simulated_2018-1} to set $T_{0}$. Specifically, given the initial solution obtained by the CH, we set the initial temperature so that a solution 50\% worse than the initial solution has a 0.5 probability for being accepted. In our experiments with small instances that includes 16-24 damaged nodes, the algorithm ends at a sufficiently small $T_{f}$ before the one-hour solution time limit is reached. However, for larger instances with 40 damaged nodes, in which neighborhood moves require more computational effort, our preliminary experiments showed that when $T_{0}$ is set at a value that allows accepting a 50\% worse solution than the initial solution, $T_{f}$ does not become sufficiently small by the end of the algorithm (i.e., within one hour). Therefore, for larger instances, $T_{0}$ is set to a value that allows accepting a 5\% worse solution than the initial solution found by the CH with 0.5 probability; in this case, a sufficiently small $T_{f}$ is reached within the one-hour solution time limit. 

To implement the proposed diversification strategy (\S 4.4), we randomly select a solution among the five best non-improving solutions generated by each neighborhood move, and evaluate the selected solution by the Metropolis criterion. For the intensification strategy, once an incumbent solution is attained, the epoch length is increased from two to five to deepen the search at the corresponding temperature. Recall that, intensification is not applied from the beginning of the algorithm since the incumbent is easily and frequently updated in the beginning, and hence implementing the intensification in the initial iterations can excessively increase the computational time. We performed preliminary experiments to determine the best point to begin implementing intensification in the algorithm and found that avoiding intensification in the first $10\%$ of the iterations of perform well. 

%The total iterations for the algorithm is estimated based on initial and final temperatures for the small instances, and on the average duration of a single iteration for the large instances.  

In our preliminary experiments, we tested three SA variants, which differ in terms of the implementation of diversification and intensification strategies. Specifically, we first evaluated the performance of the SA without any search strategies and found out that it is significantly outperformed by the SA variants with diversification and intensification. For brevity, we do not present results for the standard SA variant. We consider two SA variants in our computational study. ``$SA_{1}$'' adds only the diversification strategy on the standard SA, and in ``$SA_{2}$'' both diversification and intensification strategies are implemented. We test and compare the performances of these two SA variants in the next section. 




%DGFurthermore, in order to improve the solutions, some extensions are made to the standard application of the SA algorithm. The diversification is added in the standard application as the first method ($SA_\Romannum{1}$). Also, intensification is added in that later on ($SA_\Romannum{2}$). In both methods, due to diversification, we apply a tabu list as described in Section \ref{s:res}. There is an additional parameter here which is tabu tenure and set as 5 for all instances. Specific to intensification extension, there is a special parameter that changes the epoch length. In this phase, this parameter steps in when the obtained best solution until that moment is updated with a better one. The parameter is used for enhancing the search in that moment’s temperature. To do so, it increases the epoch length. In this problem, it is set to be doubled so that the length is changed to 4 from 2. Besides this parameter, another point that distinguishes the intensification is its starting point. We do not desire to apply intensification from beginning to end of the algorithm. The reason is that the possibility and frequency of updating the best solution reached until then is quite high in the first iterations of the algorithm. Therefore, this case increase the application time of the algorithm, which leads to loss in time. We desire to avoid from that; therefore, we set a limitation for starting the algorithm. The number of changes in the temperature value is taken. The $10\%$ of this number is calculated and used for eliminating beginning iterations. For instance, if temperature value changes for 300 times in an instance, first 30 iterations are skipped, the algorithm starts with the 31st iteration thanks to intensification.

%DG\subsection{Performance of the Simulated Annealing}
%DG\textit{Simulated Annealing} is used as the main approach. We apply the metaheuristic in two different solution strategy. There are two types of result groups which are derived from different extension approaches to SA. First, the diversification is added to standard Simulated Annealing, which we call “$SA_\Romannum{1}$”. Second, diversification and intensification are added to standard Simulated Annealing  implementation, which can be called as “$SA_\Romannum{2}$”. Similar to alternative metaheuristic algorithms, the performance of SA is considerably sensitive to the selection of the algorithm parameters. 

%Firstly, we create main algorithm parameters for both implementation methods and those are described as follows. Initial temperature is identified based on the constructive heuristic results. We use these results as the initial solutions in SA. Considering the metropolis criterion, the temperature values are calculated with the acceptance rate of $50\%$ for the results of constructive heuristic so that initial temperature is set uniquely for each instance. Terminal temperature is set as 0,001 for all instances. Afterwards, cooling rate is given as 0,90 for all instances and geometric cooling schedule is applied. Lastly, epoch length is set as 2. 


\subsection{Results: Performance of the SA Heuristic} \label{test_evaluation}

In our computational study, we solve the IIRRP model by using CPLEX and compare the solutions obtained by the solver with those of the two heuristic variants, $SA_{1}$ and $SA_{2}$. The solutions by CPLEX are obtained by imposing a one-hour solution time limit for each instance. Solving the mathematical model for the IIRRP to optimality is computationally expensive, affected mainly by the number of damaged nodes. Indeed, only a limited number of small instances with eight damaged nodes per infrastructure could be solved to optimality by CPLEX. For the rest of the instances, the average optimality gap is $43.0\%$, and minimum and maximum optimality gaps are $39.2\%$ and $46.4\%$, respectively. Objective values obtained by CPLEX, $Z_{CP}$, and the corresponding optimality gaps are reported in Tables 6 and 7 for small and large instances, respectively. 

We first compare the performances of the two SA variants on small instances. Recall that $SA_{2}$ extends $SA_{1}$ by including an intensification strategy (\S 4.4), which deepens the local search when an incumbent solution is found. Therefore, $SA_{2}$ is expected to require more computational effort compared to $SA_{1}$. We assess whether the additional computational effort leads to improved solution performance. 

Table \ref{sa_comparison} compares the solutions obtained by $SA_{1}$ and $SA_{2}$ with those of CPLEX. Since SA is a stochastic algorithm, both SA variants are run 10 times for each instance. In Table \ref{sa_comparison}, we report the minimum, average and maximum percentage gaps between the SA and the CPLEX solutions across the 10 runs. Specifically, for $SA_{2}$, Gap (\%) is calculated by $[(Z_{SA_{2}} - Z_{CP})/Z_{CP}] \times 100$. That is, a negative Gap (\%) value in the table indicates a superior SA solution compared to the solver. Furthermore, $n_{best}$ shows the number of times (out of 10 runs) that a SA variant achieves the best solution for an instance obtained by using any of the methods (i.e., CPLEX or SA variants). Finally, we present minimum, average and maximum CPU times (in seconds) of SA runs in the table.

According to the results, for each of the 16 instances that CPLEX could solve optimally (instances 5-8, 13-16, 21-24, 29-32), $SA_1$ often achieves the optimal solution several times out of 10 runs. Moreover, for these instances, average and maximum optimality gaps are low across all runs. In the remaining instances that include a larger number of damaged nodes, $SA_1$ achieves the same solution with CPLEX in 11 out of the 16 instances; and further, $SA_1$ provides better solutions than CPLEX for five instances (instances 1, 4, 12, 19 and 27). Note that in Instance 1, all runs of $SA_1$ provide better solutions compared to CPLEX.

Similar to $SA_1$, $SA_2$ can achieve the optimal solutions for the 16 instances that are solved by CPLEX optimally. Moreover, as expected, $SA_2$ can achieve better solutions than $SA_1$ in a larger number of runs. That is, adding the intensification strategy helped improve the worst case and average performance of the SA. For example, the worst solution for $SA_1$ had a gap of 2.85\%, which occurred in instance 10, and the average gap is 0.65\%. According to $SA_2$ results, maximum and average gaps for this instance decrease to 0.54\% and 0.11\%, respectively. Moreover, when intensification is added, the number of instances where all runs achieve the best solution (i.e., instances with $n_{best}=10$) increases from 1 to 7. In addition, in two instances (instances 1 and 19), $SA_2$ provides better solutions than CPLEX in all of the 10 runs.

When we compare $SA_1$ and $SA_2$ in terms of computational time, we observe that applying the proposed intensification strategy increases solution times for an average of 1006 seconds; that is, $SA_2$ requires about $25.33\%$ more time than $SA_1$. As expected, increasing the number of damaged nodes significantly affect the solution times for both SA variants. Specifically, most of instances with 16 damaged nodes could be solved within 10 minutes, whereas it takes at most about half an hour to solve instances with 24 damaged nodes. 

In summary, the results in Table 5 show that both SA variants can quickly provide high quality solutions. Moreover, $SA_2$ achieves better solutions than $SA_1$. Given that solution times of $SA_2$ are reasonable, it is worthwhile to implement $SA_2$ to solve the IIRRP. We also tested the performance of $SA_2$ on a subset of small instances (instances 1-16) by using different cooling parameters to evaluate the effects of cooling parameters on solution quality and time (Appendix C). From the results, we observe that using $\alpha=0.90$ balances the trade-off between solution quality and computational effort in our IIRRP instances.


\begin{table}[H]
    \centering
    %\tiny
    \scriptsize
\begin{tabular}{ccccccccccccccc}
\toprule
 & \multicolumn{7}{c}{$SA_1$} & \multicolumn{7}{c}{$SA_2$} \\ \cline{3-7} \cline{10-14}
 & \multicolumn{3}{c}{Gap (\%)} &  & \multicolumn{3}{c}{CPU time (s)} & \multicolumn{3}{c}{Gap (\%)} &  & \multicolumn{3}{c}{CPU time (s)} \\ \cline{2-4} \cline{6-8}  \cline{9-11} \cline{13-15} 
Inst. & min & max & avg & $n_{best}$ & min & max & avg & min & max & avg & $n_{best}$ & min & max & avg \\ \hline
\multicolumn{1}{c|}{1} & -0.98 & -0.08 & \multicolumn{1}{c|}{-0.68} & \multicolumn{1}{c|}{6} & 987 & 1,214 & \multicolumn{1}{c|}{1,126} & -0.98 & -0.17 & \multicolumn{1}{c|}{-0.52} & \multicolumn{1}{c|}{4} & 1,321 & 1,685 & 1,449 \\
\multicolumn{1}{c|}{2} & 0.00 & 2.41 & \multicolumn{1}{c|}{1.10} & \multicolumn{1}{c|}{1} & 992 & 1,217 & \multicolumn{1}{c|}{1,122} & 0.00 & 1.16 & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{2} & 1,357 & 1,757 & 1,469 \\
\multicolumn{1}{c|}{3} & 0.00 & 0.52 & \multicolumn{1}{c|}{0.25} & \multicolumn{1}{c|}{1} & 1,001 & 1,231 & \multicolumn{1}{c|}{1,129} & 0.00 & 0.51 & \multicolumn{1}{c|}{0.22} & \multicolumn{1}{c|}{3} & 1,338 & 1,770 & 1,468 \\
\multicolumn{1}{c|}{4} & -0.31 & 0.47 & \multicolumn{1}{c|}{0.11} & \multicolumn{1}{c|}{1} & 1,034 & 1,280 & \multicolumn{1}{c|}{1,179} & -0.31 & 0.47 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{2} & 1,386 & 1,711 & 1,507 \\ \hline
\multicolumn{1}{c|}{5} & 0.00 & 0.61 & \multicolumn{1}{c|}{0.16} & \multicolumn{1}{c|}{6} & 390 & 496 & \multicolumn{1}{c|}{441} & 0.00 & 0.42 & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{9} & 524 & 653 & 568 \\
\multicolumn{1}{c|}{6} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{8} & 390 & 524 & \multicolumn{1}{c|}{446} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{9} & 515 & 701 & 566 \\
\multicolumn{1}{c|}{7} & 0.00 & 0.39 & \multicolumn{1}{c|}{0.12} & \multicolumn{1}{c|}{3} & 377 & 504 & \multicolumn{1}{c|}{430} & 0.00 & 0.30 & \multicolumn{1}{c|}{0.12} & \multicolumn{1}{c|}{3} & 502 & 608 & 539 \\
\multicolumn{1}{c|}{8} & 0.00 & 0.03 & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{7} & 398 & 527 & \multicolumn{1}{c|}{456} & 0.00 & 0.64 & \multicolumn{1}{c|}{0.07} & \multicolumn{1}{c|}{8} & 489 & 604 & 526 \\ \hline
\multicolumn{1}{c|}{9} & 0.00 & 0.36 & \multicolumn{1}{c|}{0.13} & \multicolumn{1}{c|}{2} & 987 & 1,149 & \multicolumn{1}{c|}{1,105} & 0.00 & 0.33 & \multicolumn{1}{c|}{0.07} & \multicolumn{1}{c|}{4} & 1,327 & 1,615 & 1,395 \\
\multicolumn{1}{c|}{10} & 0.00 & 2.85 & \multicolumn{1}{c|}{0.65} & \multicolumn{1}{c|}{9} & 992 & 1,159 & \multicolumn{1}{c|}{1,110} & 0.00 & 0.54 & \multicolumn{1}{c|}{0.11} & \multicolumn{1}{c|}{7} & 1,347 & 1,720 & 1,449 \\
\multicolumn{1}{c|}{11} & 0.00 & 1.40 & \multicolumn{1}{c|}{0.22} & \multicolumn{1}{c|}{4} & 1,001 & 1,167 & \multicolumn{1}{c|}{1,118} & 0.00 & 0.11 & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{7} & 1,391 & 1,731 & 1,458 \\
\multicolumn{1}{c|}{12} & -0.07 & 0.39 & \multicolumn{1}{c|}{0.16} & \multicolumn{1}{c|}{1} & 1,033 & 1,231 & \multicolumn{1}{c|}{1,166} & -0.07 & 0.27 & \multicolumn{1}{c|}{0.13} & \multicolumn{1}{c|}{1} & 1,428 & 1,768 & 1,497 \\ \hline
\multicolumn{1}{c|}{13} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{7} & 389 & 455 & \multicolumn{1}{c|}{436} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 531 & 666 & 563 \\
\multicolumn{1}{c|}{14} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{8} & 387 & 454 & \multicolumn{1}{c|}{435} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{9} & 522 & 629 & 557 \\
\multicolumn{1}{c|}{15} & 0.00 & 0.29 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{5} & 376 & 445 & \multicolumn{1}{c|}{424} & 0.00 & 0.29 & \multicolumn{1}{c|}{0.06} & \multicolumn{1}{c|}{8} & 502 & 595 & 529 \\
\multicolumn{1}{c|}{16} & 0.00 & 0.31 & \multicolumn{1}{c|}{0.11} & \multicolumn{1}{c|}{6} & 396 & 462 & \multicolumn{1}{c|}{444} & 0.00 & 0.15 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{8} & 501 & 653 & 540 \\ \hline
\multicolumn{1}{c|}{17} & 0.00 & 0.27 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{6} & 1,042 & 1,349 & \multicolumn{1}{c|}{1,173} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,346 & 1,621 & 1,432 \\
\multicolumn{1}{c|}{18} & 0.00 & 0.26 & \multicolumn{1}{c|}{0.06} & \multicolumn{1}{c|}{4} & 1,034 & 1,330 & \multicolumn{1}{c|}{1,158} & 0.00 & 0.20 & \multicolumn{1}{c|}{0.08} & \multicolumn{1}{c|}{2} & 1,293 & 1,612 & 1,405 \\
\multicolumn{1}{c|}{19} & -0.70 & 0.39 & \multicolumn{1}{c|}{-0.54} & \multicolumn{1}{c|}{5} & 1,054 & 1,349 & \multicolumn{1}{c|}{1,180} & -0.70 & -0.58 & \multicolumn{1}{c|}{-0.67} & \multicolumn{1}{c|}{7} & 1,374 & 1,677 & 1,462 \\
\multicolumn{1}{c|}{20} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{1} & 1,111 & 1,409 & \multicolumn{1}{c|}{1,238} & 0.00 & 0.01 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{2} & 1,469 & 1,815 & 1,566 \\ \hline
\multicolumn{1}{c|}{21} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{7} & 390 & 502 & \multicolumn{1}{c|}{435} & 0.00 & 0.07 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{6} & 498 & 572 & 519 \\
\multicolumn{1}{c|}{22} & 0.00 & 0.25 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{7} & 403 & 516 & \multicolumn{1}{c|}{447} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 499 & 614 & 544 \\
\multicolumn{1}{c|}{23} & 0.00 & 0.51 & \multicolumn{1}{c|}{0.16} & \multicolumn{1}{c|}{5} & 419 & 537 & \multicolumn{1}{c|}{464} & 0.00 & 0.51 & \multicolumn{1}{c|}{0.21} & \multicolumn{1}{c|}{5} & 519 & 647 & 559 \\
\multicolumn{1}{c|}{24} & 0.00 & 0.19 & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{6} & 421 & 540 & \multicolumn{1}{c|}{467} & 0.00 & 0.19 & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{9} & 531 & 630 & 559 \\ \hline
\multicolumn{1}{c|}{25} & 0.00 & 0.27 & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{6} & 1,047 & 1,337 & \multicolumn{1}{c|}{1,161} & 0.00 & 0.27 & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{8} & 1,314 & 1,690 & 1,441 \\
\multicolumn{1}{c|}{26} & 0.00 & 0.06 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{4} & 1,006 & 1,320 & \multicolumn{1}{c|}{1,143} & 0.00 & 0.06 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{5} & 1,354 & 1,683 & 1,439 \\
\multicolumn{1}{c|}{27} & -0.44 & 0.22 & \multicolumn{1}{c|}{-0.26} & \multicolumn{1}{c|}{3} & 1,045 & 1,344 & \multicolumn{1}{c|}{1,167} & -0.44 & 0.11 & \multicolumn{1}{c|}{-0.24} & \multicolumn{1}{c|}{5} & 1,374 & 1,610 & 1,434 \\
\multicolumn{1}{c|}{28} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{6} & 1,113 & 1,417 & \multicolumn{1}{c|}{1,252} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,486 & 1,798 & 1,563 \\ \hline
\multicolumn{1}{c|}{29} & 0.00 & 0.08 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{6} & 391 & 503 & \multicolumn{1}{c|}{446} & 0.00 & 0.01 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{7} & 493 & 611 & 525 \\
\multicolumn{1}{c|}{30} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 401 & 517 & \multicolumn{1}{c|}{462} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 491 & 604 & 523 \\
\multicolumn{1}{c|}{31} & 0.00 & 0.10 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{7} & 419 & 538 & \multicolumn{1}{c|}{478} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 521 & 658 & 556 \\
\multicolumn{1}{c|}{32} & 0.00 & 0.34 & \multicolumn{1}{c|}{0.12} & \multicolumn{1}{c|}{3} & 405 & 543 & \multicolumn{1}{c|}{458} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 549 & 676 & 601 \\ \hline\hline 
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Avg\\ Set 1\end{tabular}} & \multicolumn{1}{c}{-0.09} & \multicolumn{1}{c}{0.64} & \multicolumn{1}{c|}{0.15} & \multicolumn{1}{c|}{4.7} & \multicolumn{1}{c}{696} & \multicolumn{1}{c}{845} & \multicolumn{1}{c|}{785} & \multicolumn{1}{c}{-0.09} & \multicolumn{1}{c}{0.33} & \multicolumn{1}{c|}{0.07} & \multicolumn{1}{c|}{5.9} & \multicolumn{1}{c}{936} & \multicolumn{1}{c}{1,179} & \multicolumn{1}{c}{1,005} \\ \hline 
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Avg\\ Set 2\end{tabular}} & \multicolumn{1}{c}{-0.07} & \multicolumn{1}{c}{0.19} & \multicolumn{1}{c|}{-0.02} & \multicolumn{1}{c|}{5.4} & \multicolumn{1}{c}{731} & \multicolumn{1}{c}{941} & \multicolumn{1}{c|}{821} & \multicolumn{1}{c}{-0.07} & \multicolumn{1}{c}{0.05} & \multicolumn{1}{c|}{-0.03} & \multicolumn{1}{c|}{7.3} & \multicolumn{1}{c}{944} & \multicolumn{1}{c}{1,157} & \multicolumn{1}{c}{1,008} \\ \hline\hline 
\multicolumn{1}{c|}{Avg} & \multicolumn{1}{c}{-0.08} & \multicolumn{1}{c}{0.42} & \multicolumn{1}{c|}{0.07} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c}{713} & \multicolumn{1}{c}{893} & \multicolumn{1}{c|}{803} & \multicolumn{1}{c}{-0.08} & \multicolumn{1}{c}{0.19} & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{6.6} & \multicolumn{1}{c}{940} & \multicolumn{1}{c}{1,168} & \multicolumn{1}{c}{1,006} \\\bottomrule
\end{tabular}
%\caption{Comparison of $SA_{1}$, $SA_{2}$ and CPLEX solutions in small instances.}
\caption{Comparison of $SA_{1}$ and $SA_{2}$ with respect to $CPLEX$ solutions in small instances.}
	\label{sa_comparison}% 
\end{table}


%\begin{figure}[http!]
%	\centering
%	\includegraphics[width= 1.1 \textwidth]{tabloGenel.jpg} \hfill
%	\caption{Boyle bir tablo. Instance 1 bir alt satirdan basliyor, yanlis cizmisim. }
%	\label{sol1}
%\end{figure}


%DGThe performance of the constructive heuristic can be compared to what CPLEX offers by observing the instances having 8 damaged nodes for each infrastructure network, where CPLEX provides $0\%$ optimality gap. The relative gaps (i.e., $[(Z_{CH} - Z_{CP})/Z_{CP}] \times 100$ (\%)) between constructive heuristic and CPLEX outcomes, which present how far is the constructive heuristic results to the CPLEX, are presented as comparison for these 16 instances, shown in Table \ref{exact_result}. The minimum and maximum of relative gaps and the average of all are $18.75\%$, $91.74\%$ and $46.05\%$, respectively. In a consequence, constructive heuristic is created as the first step of the main solution strategy, which is Simulated Annealing. We have transition of the problem from the integer programming to the metaheuristics in order to provide more appropriate methodology for the nature of the problem, which is situated in post-disaster environment.

%DG\textit{Integer Programming results} by CPLEX are obtained with a time limit of five hours is set for the run of each instance. In five hours of run, instances with 8 damaged nodes for each infrastructure reach their optimal values. However, none of the instances that have 12 damaged nodes for each infrastructure has optimal value in five hours independent of their dependency conditions. These instances’ optimality gaps are also indicated. Specically, the average optimality gap reported by CPLEX for these instances with 12 damaged gas and 12 damaged power nodes is $43.03\%$, while the minimum and maximum optimality gaps are $39.2\%$ and $46.4\%$ respectively. The main finding here is that the number of damaged nodes are one of the most effecting criteria determining the course of the problem. 

%DGTo evaluate the performance of our proposed solution method, we compare the solutions obtained by applying our constructive heuristic and metaherustic, which is Simulated Annealing, with the solutions obtained by solving the integer programming model. 

%DGThe results obtained by CPLEX and constructive heuristics for the 40-node instances are presented in Table \ref{instance_information} for both sets. In the table, instance numbers are listed. For each of them, the resulted objective values from CPLEX and regarding optimality gaps, as well as their results from constructive heuristic are presented and compared by percentage.

%To evaluate the performance of the proposed solution method, we compare the solutions obtained by applying our constructive heuristic and metaherustic, which is Simulated Annealing, with the solutions obtained by solving the integer programming model. In our IP computations, we run IBM ILOG OPL/CPLEX on a 64-bit Windows Server with two 2.0 GHz Intel Xeon CPU's and 32 GB RAM.  (Burcu hoca’nın makalesine göre editlenebilir)


%\textit{Constructive heuristic results} are obtained with a greedy approach as described in Section \ref{s:res}. Earlier, we observe the results gathered from CPLEX($Z_{CP}$) and report that the mathematical model cannot provide us with the optimal solutions in time limit as the number of damaged nodes are increased and the environment of the infrastructure network is getting more complex. The set of time limit of five hours in CPLEX is quite adequate when we consider the problem itself requires an instant remedy due to its post-disaster environment. More precisely, such problems having post-disaster environments are in urgent need of a more time-efficient strategy which requires simplified tools and techniques and also offers better results. However, in an adequate time frame, CPLEX cannot give the optimal solution in complex circumstances of having 12 damaged nodes for each infrastructure network. Therefore, in order to offer more suitable approach for the problem environment, we create a constructive heuristic and lay the foundations of a metaheuristic approach.

%     \begin{table}[H]
%     \footnotesize
%     \begin{tabular}{cccccccccc}
%     \hline
%     \multicolumn{1}{l}{} & \multicolumn{2}{c}{CPLEX} & \multicolumn{2}{c}{Heuristic} &  & \multicolumn{2}{c}{CPLEX} & \multicolumn{2}{c}{Heuristic} \\ \hline
%     \multicolumn{1}{c|}{Instance} & $Z_{CP}$ & O.Gap(\%) & $Z_{CH}$ & \multicolumn{1}{c|}{Gap(\%)} & \multicolumn{1}{c|}{Instance} & $Z_{Cp}$ & O.Gap(\%) & $Z_{CH}$ & Gap(\%) \\
%     \multicolumn{1}{c|}{1} & 4763.1 & 43.9 & 6480.3 & \multicolumn{1}{c|}{36.1} & \multicolumn{1}{c|}{17} & 5318.5 & 42.3 & 7287.0 & 37.0 \\
%     \multicolumn{1}{c|}{2} & 4899.5 & 44.6 & 6389.9 & \multicolumn{1}{c|}{30.4} & \multicolumn{1}{c|}{18} & 4846.3 & 45.7 & 6553.9 & 35.2 \\
%     \multicolumn{1}{c|}{3} & 5467.5 & 40.4 & 7165.2 & \multicolumn{1}{c|}{31.1} & \multicolumn{1}{c|}{19} & 5649.7 & 43.2 & 6975.3 & 23.5 \\
%     \multicolumn{1}{c|}{4} & 5369.6 & 39.2 & 8747.5 & \multicolumn{1}{c|}{62.9} & \multicolumn{1}{c|}{20} & 6106.8 & 43.4 & 11181.3 & 83.1 \\
%     \multicolumn{1}{c|}{5} & 2501.6 & 0.0 & 2970.7 & \multicolumn{1}{c|}{18.8} & \multicolumn{1}{c|}{21} & 2181.6 & 0.0 & 2866.5 & 31.4 \\
%     \multicolumn{1}{c|}{6} & 2729.3 & 0.0 & 4589.7 & \multicolumn{1}{c|}{68.2} & \multicolumn{1}{c|}{22} & 2743.3 & 0.0 & 3462.8 & 26.2 \\
%     \multicolumn{1}{c|}{7} & 2422.4 & 0.0 & 3239.8 & \multicolumn{1}{c|}{33.7} & \multicolumn{1}{c|}{23} & 3113.4 & 0.0 & 4921.9 & 58.1 \\
%     \multicolumn{1}{c|}{8} & 3510.5 & 0.0 & 5145.8 & \multicolumn{1}{c|}{46.6} & \multicolumn{1}{c|}{24} & 3126.5 & 0.0 & 5994.6 & 91.7 \\
%     \multicolumn{1}{c|}{9} & 4794.0 & 46.4 & 6684.8 & \multicolumn{1}{c|}{39.4} & \multicolumn{1}{c|}{25} & 5415.2 & 42.0 & 7282.7 & 34.5 \\
%     \multicolumn{1}{c|}{10} & 4971.9 & 44.0 & 6477.8 & \multicolumn{1}{c|}{30.3} & \multicolumn{1}{c|}{26} & 4838.9 & 43.3 & 6697.1 & 38.4 \\
%     \multicolumn{1}{c|}{11} & 5466.8 & 40.3 & 7331.3 & \multicolumn{1}{c|}{34.1} & \multicolumn{1}{c|}{27} & 5617 & 44.2 & 7179.9 & 27.8 \\
%     \multicolumn{1}{c|}{12} & 5499.8 & 43.5 & 8538.6 & \multicolumn{1}{c|}{55.3} & \multicolumn{1}{c|}{28} & 5886.5 & 42.0 & 10995.0 & 86.8 \\
%     \multicolumn{1}{c|}{13} & 2609.8 & 0.0 & 3118.5 & \multicolumn{1}{c|}{19.5} & \multicolumn{1}{c|}{29} & 2167.2 & 0.0 & 2831.0 & 30.6 \\
%     \multicolumn{1}{c|}{14} & 2663.7 & 0.0 & 4480.3 & \multicolumn{1}{c|}{68.2} & \multicolumn{1}{c|}{30} & 2726.2 & 0.0 & 3519.3 & 29.1 \\
%     \multicolumn{1}{c|}{15} & 2371.5 & 0.0 & 3216.3 & \multicolumn{1}{c|}{35.6} & \multicolumn{1}{c|}{31} & 3081.1 & 0.0 & 4701.8 & 52.6 \\
%     \multicolumn{1}{c|}{16} & 3300.9 & 0.0 & 4800.7 & \multicolumn{1}{c|}{45.4} & \multicolumn{1}{c|}{32} & 3169.8 & 0.0 & 5739.9 & 81.1 \\ \hline
% \end{tabular}
% \caption{.}
% 	\label{exact_result}% 
% \end{table}



%DGThe results obtained by implementing Simulated Annealing with both strategies $SA_\Romannum{1}$ and $SA_\Romannum{2}$ are presented in Table \ref{sa_comparison}. For all instances, both implementations are run 10 times. The SA algorithm is stochastic and therefore; algorithm provides different outcomes in each run despite having the same factors and parameters. Hence, the algorithms are run 10 times for each instance. Afterwards, results are compared with the presentation of minimum, maximum and average results for both implementations of SA. The relative gaps with CPLEX outcomes for all minimum, maximum and average values are also presented.



%DGFirstly, considering the results obtained by first implementation strategy $SA_\Romannum{1}$, they can be compared with the optimal results generated from CPLEX. We examine the 16 instances with less damaged nodes where CPLEX reaches optimal solutions. For all 16 instances, the best solutions among 10 run of the first heuristic strategy are the same with the optimal values that CPLEX provides. We also examine the other 16 instances with more damaged nodes that CPLEX cannot provide optimal results. According to the best heuristic results, 11 of these 16 instances have the same value with the CPLEX. Other 5 instances have  better solutions than CPLEX results thanks to SA extended with diversification. For example, instance with id 1 has objective value of 4716.6 as a result of SA, where CPLEX provides 4763.1. The relative gap is $-0.98\%$, showing that metaheuristic approach enables better solution than mathematical model in this case.



%We analyze the results obtained by second implementation strategy $SA_\Romannum{2}$. As indicated earlier, it is a more extended version which covers both diversification and intensification. Examining the performance of $SA_\Romannum{2}$, the results can be compared with $SA_\Romannum{1}$. The best results among 10 run for each instance are the same with the previous SA implementation, $SA_\Romannum{1}$. What is better is that the worst results among 10 times run of the instances are improved. The worst outcomes among 10 are better in this implementation so that the gap between the best and the worst result among 10 trial become smaller. Analyzing the \nth{10} instance as an example, considering the first implementation ($SA_\Romannum{1}$) which extended with mere diversification, the worst result among 10 times run has relative gap (i.e., $[(Z_{SA_\Romannum{1}} - Z_{CP})/Z_{CP}] \times 100$ (\%)) of $2,85\%$ with CPLEX and the average relative gap of for all runs is $0.65\%$. In this time, by adding intensification, the relative gaps (i.e., $[(Z_{SA_\Romannum{2}} - Z_{CP})/Z_{CP}] \times 100$ (\%)) change to $0.54\%$ and $0.11\%$ respectively. Another improvement is that there is only one instance which we have the same (the best) result for all 10 run in the first SA implementation. The number rises up to 7 thanks to intensification added $SA_\Romannum{2}$. In addition to that, there is again only one instance (\nth{1} instance) that we have better results than CPLEX for all 10 run, this number increases to 2 (\nth{1} and \nth{19} instances) in this method. 




Table 6 compares the solution values obtained by CPLEX, the constructive heuristic (CH) and the best solution found by $SA_{2}$ out of 10 runs for the small instances. We report the optimality gap reported by CPLEX, as well as the percentage gap between our algorithms' solutions and CPLEX. For example, for instance 1, the best objective value found by $SA_{2}$ is 4716.6, which is 0.98\% better than the best solution, 4763.1, achieved by CPLEX within the one-hour time limit. Note that $SA_{2}$ achieves this solution four times out of 10 runs (Table 5), and all other runs obtain a better solution than CPLEX. Given that $SA_{2}$ can achieve optimal solutions and sometimes better solutions than CPLEX, and the worst percentage gap of $SA_{2}$ is within 0.64\%, we can suggest that $SA_{2}$ can be used in practice to obtain high quality and quick solutions. From Table 6 results, we also observe that the performance of CH is reasonable in some instances, providing a gap within 20\% of the best or optimal solution; however, in some instances the gap can be large, exceeding 30\%. $SA_2$ significantly improves the CH solutions in all instances.



\begin{table}[H]
    \footnotesize
    \centering
\begin{tabular}{@{}ccccccc@{}}
\toprule
%         & \multicolumn{2}{c}{$CPLEX$} & \multicolumn{2}{c}{$CH$} & \multicolumn{2}{c}{$SA_2$} \\ \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7}
Instance & $Z_{CP}$      & O.Gap (\%) & $Z_{CH}$                 & Gap (\%)      & $Z_{SA_2}$  & Gap (\%) \\ \midrule
\multicolumn{1}{c|}{1}        & 4,763.1       & 43.90      & 6,072.1                   & 27.48          & 4,716.6     & -0.98    \\
\multicolumn{1}{c|}{2}        & 4,899.5       & 44.60      & 6,265.0                   & 27.87          & 4,899.5     & 0.00        \\
\multicolumn{1}{c|}{3}        & 5,467.5       & 40.40      & 6,679.6                   & 22.17          & 5,467.5     & 0.00        \\
\multicolumn{1}{c|}{4}        & 5,369.6       & 39.20      & 6,903.6                   & 28.57          & 5,353.1     & -0.31    \\
\multicolumn{1}{c|}{5}        & 2,501.6       & 0.00      & 2,855.6                   & 14.15          & 2,501.6     & 0.00        \\
\multicolumn{1}{c|}{6}        & 2,729.3       & 0.00      & 2,965.3                   & 8.65          & 2,729.3     & 0.00        \\
\multicolumn{1}{c|}{7}        & 2,422.4       & 0.00      & 2,734.8                   & 12.90          & 2,422.4     & 0.00        \\
\multicolumn{1}{c|}{8}        & 3,510.5       & 0.00      & 5,600.6                   & 59.54          & 3,510.5     & 0.00        \\
\multicolumn{1}{c|}{9}        & 4,794.0       & 46.40      & 6,220.1                   & 29.75          & 4,794.0     & 0.00        \\
\multicolumn{1}{c|}{10}       & 4,971.9       & 44.00      & 6,324.7                   & 27.21          & 4,971.9     & 0.00        \\
\multicolumn{1}{c|}{11}       & 5,466.8       & 40.30      & 6,868.7                   & 25.64          & 5,466.8     & 0.00        \\
\multicolumn{1}{c|}{12}       & 5,499.8       & 43.50      & 6,959.7                   & 26.54          & 5,495.8     & -0.07    \\
\multicolumn{1}{c|}{13}       & 2,609.8       & 0.00      & 3,025.7                   & 15.94          & 2,609.8     & 0.00        \\
\multicolumn{1}{c|}{14}       & 2,663.7       & 0.00      & 2,860.7                   & 7.40          & 2,663.7     & 0.00        \\
\multicolumn{1}{c|}{15}       & 2,371.5       & 0.00      & 2,723.8                   & 14.86          & 2,371.5     & 0.00        \\
\multicolumn{1}{c|}{16}       & 3,300.9       & 0.00      & 5,123.3                   & 55.21          & 3,300.9     & 0.00      \\
\multicolumn{1}{c|}{17}       & 5,318.5       & 42.30      & 6,637.2                   & 24.79          & 5,318.5     & 0.00    \\
\multicolumn{1}{c|}{18}       & 4,846.3       & 45.70      & 6,358.5                   & 31.20          & 4,846.3     & 0.00    \\
\multicolumn{1}{c|}{19}       & 5,649.7       & 43.20      & 7,342.7                   & 29.97          & 5,609.9     & -0.70    \\
\multicolumn{1}{c|}{20}       & 6,106.8       & 43.40      & 8,412.8                   & 37.76          & 6,106.8     & 0.00        \\
\multicolumn{1}{c|}{21}       & 2,181.6       & 0.00      & 2,708.7                   & 24.16          & 2,181.6     & 0.00        \\
\multicolumn{1}{c|}{22}       & 2,743.3       & 0.00      & 3,455.0                   & 25.94          & 2,743.3     & 0.00        \\
\multicolumn{1}{c|}{23}       & 3,113.4       & 0.00      & 3,591.8                   & 15.37          & 3,113.4     & 0.00        \\
\multicolumn{1}{c|}{24}       & 3,126.5       & 0.00      & 3,843.4                   & 22.93          & 3,126.5     & 0.00        \\
\multicolumn{1}{c|}{25}       & 5,415.2       & 42.00      & 6,795.4                   & 25.49          & 5,415.2     & 0.00        \\
\multicolumn{1}{c|}{26}       & 4,838.9       & 43.30      & 6,529.4                   & 34.94          & 4,838.9     & 0.00        \\
\multicolumn{1}{c|}{27}       & 5,617.0       & 44.20      & 7,485.8                   & 33.27          & 5,592.4     & -0.44    \\
\multicolumn{1}{c|}{28}       & 5,886.5       & 42.00      & 8,085.8                   & 37.36          & 5,886.5     & 0.00     \\
\multicolumn{1}{c|}{29}       & 2,167.2       & 0.00      & 2,688.4                   & 24.05          & 2,167.2     & 0.00     \\
\multicolumn{1}{c|}{30}       & 2,726.2       & 0.00      & 3,507.8                   & 28.67          & 2,726.2     & 0.00     \\
\multicolumn{1}{c|}{31}       & 3,081.1       & 0.00      & 3,590.0                   & 16.52          & 3,081.1     & 0.00     \\
\multicolumn{1}{c|}{32}       & 3,169.8       & 0.00      & 3,695.0                   & 16.67          & 3,169.8     & 0.00     \\ \bottomrule
\end{tabular}
\caption{Comparison of $CPLEX$, $CH$ and $SA_{2}$ solutions in small instances.}
	\label{cplex_results}% 
\end{table}


%Furthermore, if we analyze at how many of the 10 run performed for all instances give the best results, the average number is 5 for the $SA_\Romannum{1}$. This number increased to 7 with the $SA_\Romannum{2}$.

%Finally, it should be mentioned that we require more time compared to $SA_\Romannum{1}$ in this case since it has a wider search in the solution space. Comparing the average time spent for both SA implementations, $SA_\Romannum{2}$ requires 1.006 seconds in total, which is $25.33\%$ higher than the $SA_\Romannum{1}$. The instances with 8 damaged nodes for each infrastructure have 548 (vs. 448) seconds run time. The others with 12 damaged nodes for each infrastructure have 1.465 (vs. 1168) seconds. 

%In a nutshell, $SA_\Romannum{2}$, extending SA with both diversification and intensification, provides better results depending on different comparisons and analysis. The fact  that it requires more time in search can be compensated with the obtained better results than not only CPLEX but also the $SA_\Romannum{1}$.


Lastly, we evaluate the performances of $SA_2$ and CH in solving larger instances (given in Table \ref{_largeinstance_information}). Table \ref{large_result} presents the solutions obtained by CPLEX within one hour time limit and the corresponding optimality gaps, the CH solution value and its percentage gap with respect to CPLEX solution, and the minimum, average and maximum solution values of $SA_2$ out of 10 runs and their comparison with CPLEX solutions in terms of percentage gaps. According to the results, almost all runs of $SA_2$ perform better than CPLEX for all instances. Note that only in one instance (instance 43), $SA_2$ runs lead to a solution value (in the worst run) slightly larger than that of CPLEX. On average, $SA_2$ performs 1.54\% better than CPLEX over all large instances. We also observe that although CH provides reasonable initial feasible solutions for the IIRRP very quickly, the solutions can be significantly improved by $SA_2$. Overall, our analyses indicate that a single run of $SA_2$ could be sufficient to support repair planning efforts.

\subsection{Results: Effects of Coordinated Planning}
In this section, we present the results of numerical experiments performed to evaluate the benefits of planning repair routes of the two infrastructure networks in a coordinated way. The illustrative example in \S 3.1.4, indicated that ignoring the interdependencies between the two infrastructures may highly effect the recovery times. Here, we further perform a similar analysis for all small instances to evaluate the effects of coordinated planning on infrastructure recovery times. 

\begin{table}[H]
    \footnotesize
    \centering
\begin{tabular}{ccc ccc ccc cc}
\toprule
% & \multicolumn{2}{c}{$CPLEX$} & \multicolumn{2}{c}{$CH$} & \multicolumn{3}{c}{$Z_{SA_2}$} & \multicolumn{3}{c}{Gap (\%)} \\ \cmidrule(l){2-11} 
% Inst. & $Z_{CP}$ & O.Gap (\%) & $Z_{CH}$ & Gap (\%) & min & avg & max & min & avg & max \\ \midrule
&  &  &  &  & \multicolumn{3}{c}{$Z_{SA_2}$} & \multicolumn{3}{c}{Gap (\%)} \\ \cmidrule(l){6-11} 
Inst. & $Z_{CP}$ & O.Gap (\%) & $Z_{CH}$ & Gap (\%) & min & avg & max & min & avg & max \\ \midrule
\multicolumn{1}{c|}{33} & 12,915.4 & 68.71 & 17,052.4 & 32.03 & 12,720.2 & 12,752.1 & 12,763.7 & -1.51 & -1.26 & -1.17 \\
\multicolumn{1}{c|}{34} & 14,203.8 & 70.33 & 19,003.9 & 33.79 & 13,807.7 & 13,908.6 & 14,053.9 & -2.79 & -2.08 & -1.06 \\
\multicolumn{1}{c|}{35} & 14,351.0 & 69.94 & 19,237.7 & 34.05 & 14,245.9 & 14,289.7 & 14,310.8 & -0.73 & -0.43 & -0.28 \\
\multicolumn{1}{c|}{36} & 18,795.8 & 78.79 & 23,770.8 & 26.47 & 17,992.5 & 18,104.5 & 18,174.9 & -4.27 & -3.68 & -3.30 \\
\multicolumn{1}{c|}{37} & 19,921.5 & 77.94 & 26,438.0 & 32.71 & 19,683.6 & 19,771.1 & 19,886.5 & -1.19 & -0.75 & -0.18 \\
\multicolumn{1}{c|}{38} & 20,024.3 & 77.45 & 26,301.6 & 31.35 & 19,190.4 & 19,316.4 & 19,417.5 & -4.16 & -3.54 & -3.03 \\
\multicolumn{1}{c|}{39} & 9,113.6 & 63.48 & 11,130.0 & 22.13 & 8,887.4 & 8,907.0 & 8,923.3 & -2.48 & -2.27 & -2.09 \\
\multicolumn{1}{c|}{40} & 9,733.5 & 57.60 & 12,279.7 & 26.16 & 9,626.8 & 9,634.6 & 9,655.7 & -1.10 & -1.02 & -0.80 \\
\multicolumn{1}{c|}{41} & 10,020.9 & 56.79 & 14,079.0 & 40.50 & 9,971.0 & 9,987.0 & 10,015.2 & -0.50 & -0.34 & -0.06 \\
\multicolumn{1}{c|}{42} & 12,875.4 & 71.46 & 16,692.4 & 29.65 & 12,553.5 & 12,742.2 & 12,858.7 & -2.50 & -1.03 & -0.13 \\
\multicolumn{1}{c|}{43} & 13,824.2 & 69.03 & 17,718.6 & 28.17 & 13,741.5 & 13,786.4 & 13,829.0 & -0.60 & -0.27 & 0.03 \\
\multicolumn{1}{c|}{44} & 13,820.0 & 68.38 & 17,488.7 & 26.55 &13,389.3 & 13,569.9 & 13,693.1 & -3.12 & -1.81 & -0.92 \\ \hline\hline
\multicolumn{1}{c|}{Avg} &  & 69.16 &  & 30.30 &  &  &  & -2.08 & -1.54 & -1.08 \\ \bottomrule
%\multicolumn{1}{c|}{Avg} & 14,133.3 & 69.16 & 18,432.7 & 30.30 & 13,818.0 & 13,897.5 & 13,965.2 & -2.08 & -1.54 & -1.08 \\ \bottomrule
\end{tabular}
\caption{Comparison of $CPLEX$ and $SA_{2}$ solutions in large instances.}
	\label{large_result}% 
\end{table}

Table 8 reports total recovery times obtained in uncoordinated and coordinated cases, and presents the percentage gap between these two solutions, represented by $\Delta$. That is, $\Delta$ shows how much improvement we can obtain by coordinated planning. To calculate the total recovery times in the uncoordinated case (i.e., Case II in \S 3.1.4), we solve the mathematical model by CPLEX separately for each network by ignoring the interdependencies between the power and gas networks. That is, we assume that the organizations responsible for repairs of different infrastructures only consider the interdependence within their own networks, and ignore how their repair plans may affect the recovery times of the other infrastructure. In the coordinated case, we take all interdependencies into account and solve the problem by using $SA_2$. That is, the coordinated case assumes that a central decision maker develops repair plans for both infrastructures. In Table 8, the recovery times from the coordinated case are the best $SA_2$ solutions obtained for the small instances, which are also reported in Table 6. 


According to the results, coordinating repair efforts lead to $5.80\%$ improvement in total recovery times on average. Furthermore, the maximum improvement is $21.28\%$. Given the importance and time-sensitive nature of post-disaster infrastructure repair operations, these differences may have important implications on the survival of people and recovery of the society. Furthermore, we observe that $\Delta$ values tend to be larger in instances that include a larger number of interdependent links (i.e., larger $\delta_{pg}$ and $\delta_{gp}$ values) (e.g., instances 8, 12, 16, 24). As expected, the effects of coordinated planning on recovery times can be more significant when more components in different infrastructure networks depend on each other. However, results also show that, a large number of interdependent components does not always lead to large $\Delta$ values (e.g., instances 4, 28, 32). This is because the network structure (i.e., how the component dependencies are positioned in and between infrastructures), and the location of damaged components also affect the recovery times, which may be difficult to assess without the help of any systematic tool. Therefore, the proposed mathematical model and the efficient heuristics can highly support decision makers to evaluate the effects of coordinated repair planning in the field and develop repair plans in both coordinated and uncoordinated settings. 



% Burayı yukarıdaki şekilde güncelledim; First, we solve the mathematical model separately for each network by ignoring inter-network dependencies. However, the total completion time resulted from ignoring the inter-network dependencies is not very realistic due to nature of the networks since they have dependencies among different infrastructures in real life scenarios. In order to see at what scale the reality of having inter-network dependencies affect the problem, we keep the route resulted from eliminating the inter-network dependencies in CPLEX. Using the same route, the problem is resolved by taking all dependencies into account. Then, the total completion time is recalculated and results are presented in Table \ref{coord_vs_uncoord}.

% Bu paragrafı da yeni yazdığım paragrafa entegre etmiş oldum; Coordinated completion times are obtained using our metaheuristic approach (which are the best results of second extended SA implementation for 32 instances. In Table \ref{coord_vs_uncoord}, in addition to completion times in uncoordinated cases, we also present the resulted times in coordinated cases for all 32 instances. The percentage differences between these two solutions are also indicated. 

%Burayı çıkarıyoruz; As an example, if we examine \nth{12} instance as considering inter-network dependencies, we see that their routes are different in coordinated and uncoordinated circumstances. Undoubtedly, the difference is resulted from inter-network dependencies (route tablo ref). The visualization with the dependency structure for \nth{12} instance can be seen in Figure XXX. When power to gas dependencies are considered, we can see the effect of \nth{9} power node to broken \nth{10} and \nth{20} gas nodes. In fact, this effect influences the entire network. To exemplify, the \nth{10} gas node which is affected in this case has also effect on not only \nth{15} and \nth{16} gas nodes but also the \nth{4} power node. Here, by creating a chain effect in the network, the effected \nth{15} gas node has impact on the \nth{9} power node, which is the first node we examined. Dependencies maintain their existences in this way so that they have effect on not only the following node that is in their circle but also the whole network. Nonetheless, if we examine the routes resulted from coordinated cases, we notice that the \nth{9} power node, one of the nodes leading to these interactions, is visited in the first place. However, it is not the first node to be visited in the solution of uncoordinated case. As a result, completion time of \nth{9} power node is 293.3 in the solution gathered from the uncoordinated network, yet it becomes 127.9 in case of it is visited in the first place. Considering the coordinated solution along with other changes in the routes, it provides a 21.28\% benefit as a consequence.



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[H]
% \centering
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% \multicolumn{1}{l}{} & \multicolumn{3}{c}{Uncoordinated} & \multicolumn{2}{c}{Coordinated} \\ \midrule
% \multicolumn{1}{c|}{Instance} & \begin{tabular}[c]{@{}c@{}}Objective Values \\ without g-p and p-g \\ dependency\end{tabular} & \begin{tabular}[c]{@{}c@{}}Objective Values \\ with g-p and p-g \\ dependency\end{tabular} & \multicolumn{1}{c|}{Diff.(\%)} & \begin{tabular}[c]{@{}c@{}}SA\\ Result\end{tabular} & Diff.(\%) \\ \midrule
% \multicolumn{1}{c|}{1} & 2307.3 & 2658.5 & \multicolumn{1}{c|}{15.22} & 2501.6 & 5.90 \\
% \multicolumn{1}{c|}{2} & 2658.5 & 2753.5 & \multicolumn{1}{c|}{3.57} & 2729.3 & 0.88 \\
% \multicolumn{1}{c|}{3} & 2390.2 & 2578.5 & \multicolumn{1}{c|}{7.88} & 2422.4 & 6.05 \\
% \multicolumn{1}{c|}{4} & 2712.4 & 4274.6 & \multicolumn{1}{c|}{57.59} & 3510.5 & 17.88 \\
% \multicolumn{1}{c|}{5} & 4716.6 & 4716.6 & \multicolumn{1}{c|}{0.00} & 4716.6 & 0.00 \\
% \multicolumn{1}{c|}{6} & 4582.3 & 5599.9 & \multicolumn{1}{c|}{22.21} & 4899.5 & 12.51 \\
% \multicolumn{1}{c|}{7} & 4949 & 5645.1 & \multicolumn{1}{c|}{14.07} & 5467.5 & 3.15 \\
% \multicolumn{1}{c|}{8} & 4962.3 & 5466.7 & \multicolumn{1}{c|}{10.16} & 5353.1 & 2.08 \\
% \multicolumn{1}{c|}{9} & 2413.9 & 2792.7 & \multicolumn{1}{c|}{15.69} & 2609.8 & 6.55 \\
% \multicolumn{1}{c|}{10} & 2457.6 & 2681.7 & \multicolumn{1}{c|}{9.12} & 2663.7 & 0.67 \\
% \multicolumn{1}{c|}{11} & 2326.2 & 2525.6 & \multicolumn{1}{c|}{8.57} & 2371.5 & 6.10 \\
% \multicolumn{1}{c|}{12} & 2572.3 & 4193.3 & \multicolumn{1}{c|}{63.02} & 3300.9 & 21.28 \\
% \multicolumn{1}{c|}{13} & 4802.6 & 4802.6 & \multicolumn{1}{c|}{0.00} & 4794 & 0.18 \\
% \multicolumn{1}{c|}{14} & 4657.6 & 5772.1 & \multicolumn{1}{c|}{23.93} & 4971.9 & 13.86 \\
% \multicolumn{1}{c|}{15} & 4936.4 & 5731.7 & \multicolumn{1}{c|}{16.11} & 5466.8 & 4.62 \\
% \multicolumn{1}{c|}{16} & 5104.2 & 5925.7 & \multicolumn{1}{c|}{16.09} & 5495.8 & 7.25 \\
% \multicolumn{1}{c|}{17} & 2173.8 & 2271.3 & \multicolumn{1}{c|}{4.49} & 2181.6 & 3.95 \\
% \multicolumn{1}{c|}{18} & 2353.5 & 3029.1 & \multicolumn{1}{c|}{28.71} & 2743.3 & 9.44 \\
% \multicolumn{1}{c|}{19} & 2412.6 & 3447.8 & \multicolumn{1}{c|}{42.91} & 3113.4 & 9.70 \\
% \multicolumn{1}{c|}{20} & 2702.1 & 3370.9 & \multicolumn{1}{c|}{24.75} & 3126.5 & 7.25 \\
% \multicolumn{1}{c|}{21} & 5215.3 & 5464.3 & \multicolumn{1}{c|}{4.77} & 5318.5 & 2.67 \\
% \multicolumn{1}{c|}{22} & 4846.3 & 4846.3 & \multicolumn{1}{c|}{0.00} & 4846.3 & 0.00 \\
% \multicolumn{1}{c|}{23} & 5127.9 & 6069.2 & \multicolumn{1}{c|}{18.36} & 5609.9 & 7.57 \\
% \multicolumn{1}{c|}{24} & 5703.1 & 6344.6 & \multicolumn{1}{c|}{11.25} & 6106.8 & 3.75 \\
% \multicolumn{1}{c|}{25} & 2152 & 2249.6 & \multicolumn{1}{c|}{4.54} & 2167.2 & 3.66 \\
% \multicolumn{1}{c|}{26} & 2333.7 & 3027.4 & \multicolumn{1}{c|}{29.73} & 2726.2 & 9.95 \\
% \multicolumn{1}{c|}{27} & 2345.1 & 3313.4 & \multicolumn{1}{c|}{41.29} & 3081.1 & 7.01 \\
% \multicolumn{1}{c|}{28} & 2707.3 & 3334.1 & \multicolumn{1}{c|}{23.15} & 3169.8 & 4.93 \\
% \multicolumn{1}{c|}{29} & 5277.8 & 5648.3 & \multicolumn{1}{c|}{7.02} & 5415.2 & 4.13 \\
% \multicolumn{1}{c|}{30} & 4838.9 & 4838.9 & \multicolumn{1}{c|}{0.00} & 4838.9 & 0.00 \\
% \multicolumn{1}{c|}{31} & 5092.7 & 5707.3 & \multicolumn{1}{c|}{12.07} & 5592.4 & 2.01 \\
% \multicolumn{1}{c|}{32} & 5486.2 & 5918.8 & \multicolumn{1}{c|}{7.89} & 5886.5 & 0.55 \\ \bottomrule
% \end{tabular}
%     \caption{Comparison of total completion times under coordinated and uncoordinated planning.}
%     \label{coord_vs_uncoord}%
% \end{table}


\begin{table}[H]
    \footnotesize
    \centering
\begin{tabular}{@{}cccr@{}}
\toprule
 & \multicolumn{2}{c}{Recovery Times} &  \\ \cmidrule(lr){2-3}
\multicolumn{1}{c|}{Instance} & \multicolumn{1}{c|}{Uncoordinated} & \multicolumn{1}{c|}{Coordinated} & $\Delta (\%)$ \\ \midrule
\multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{4,716.6} & \multicolumn{1}{c|}{4,716.6} & 0.00 \\
\multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{5,599.9} & \multicolumn{1}{c|}{4,899.5} & 12.51 \\
\multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{5,645.1} & \multicolumn{1}{c|}{5,467.5} & 3.15 \\
\multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5,466.7} & \multicolumn{1}{c|}{5,353.1} & 2.08 \\
\multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{2,658.5} & \multicolumn{1}{c|}{2,501.6} & 5.90 \\
\multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{2,753.5} & \multicolumn{1}{c|}{2,729.3} & 0.88 \\
\multicolumn{1}{c|}{7} & \multicolumn{1}{c|}{2,578.5} & \multicolumn{1}{c|}{2,422.4} & 6.05 \\
\multicolumn{1}{c|}{8} & \multicolumn{1}{c|}{4,274.6} & \multicolumn{1}{c|}{3,510.5} & 17.88 \\
\multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{4,802.6} & \multicolumn{1}{c|}{4,794.0} & 0.18 \\
\multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{5,772.1} & \multicolumn{1}{c|}{4,971.9} & 13.86 \\
\multicolumn{1}{c|}{11} & \multicolumn{1}{c|}{5,731.7} & \multicolumn{1}{c|}{5,466.8} & 4.62 \\
\multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{5,925.7} & \multicolumn{1}{c|}{5,495.8} & 7.25 \\
\multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{2,792.7} & \multicolumn{1}{c|}{2,609.8} & 6.55 \\
\multicolumn{1}{c|}{14} & \multicolumn{1}{c|}{2,681.7} & \multicolumn{1}{c|}{2,663.7} & 0.67 \\
\multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{2,525.6} & \multicolumn{1}{c|}{2,371.5} & 6.10 \\
\multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{4,193.3} & \multicolumn{1}{c|}{3,300.9} & 21.28 \\
\multicolumn{1}{c|}{17} & \multicolumn{1}{c|}{5,464.3} & \multicolumn{1}{c|}{5,318.5} & 2.67 \\
\multicolumn{1}{c|}{18} & \multicolumn{1}{c|}{4,846.3} & \multicolumn{1}{c|}{4,846.3} & 0.00 \\
\multicolumn{1}{c|}{19} & \multicolumn{1}{c|}{6,069.2} & \multicolumn{1}{c|}{5,609.9} & 7.57 \\
\multicolumn{1}{c|}{20} & \multicolumn{1}{c|}{6,344.6} & \multicolumn{1}{c|}{6,106.8} & 3.75 \\
\multicolumn{1}{c|}{21} & \multicolumn{1}{c|}{2,271.3} & \multicolumn{1}{c|}{2,181.6} & 3.95 \\
\multicolumn{1}{c|}{22} & \multicolumn{1}{c|}{3,029.1} & \multicolumn{1}{c|}{2,743.3} & 9.44 \\
\multicolumn{1}{c|}{23} & \multicolumn{1}{c|}{3,447.8} & \multicolumn{1}{c|}{3,113.4} & 9.70 \\
\multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{3,370.9} & \multicolumn{1}{c|}{3,126.5} & 7.25 \\
\multicolumn{1}{c|}{25} & \multicolumn{1}{c|}{5,648.3} & \multicolumn{1}{c|}{5,415.2} & 4.13 \\
\multicolumn{1}{c|}{26} & \multicolumn{1}{c|}{4,838.9} & \multicolumn{1}{c|}{4,838.9} & 0.00 \\
\multicolumn{1}{c|}{27} & \multicolumn{1}{c|}{5,707.3} & \multicolumn{1}{c|}{5,592.4} & 2.01 \\
\multicolumn{1}{c|}{28} & \multicolumn{1}{c|}{5,918.8} & \multicolumn{1}{c|}{5,886.5} & 0.55 \\
\multicolumn{1}{c|}{29} & \multicolumn{1}{c|}{2,249.6} & \multicolumn{1}{c|}{2,167.2} & 3.66 \\
\multicolumn{1}{c|}{30} & \multicolumn{1}{c|}{3,027.4} & \multicolumn{1}{c|}{2,726.2} & 9.95 \\
\multicolumn{1}{c|}{31} & \multicolumn{1}{c|}{3,313.4} & \multicolumn{1}{c|}{3,081.1} & 7.01 \\
\multicolumn{1}{c|}{32} & \multicolumn{1}{c|}{3,334.1} & \multicolumn{1}{c|}{3,169.8} & 4.93 \\ \bottomrule
\end{tabular}
    \caption{Comparison of total recovery times under coordinated and uncoordinated planning.}
    \label{coord_vs_uncoord}%
\end{table}



% \begin{table}[H]
%     \footnotesize
%     \centering
% \begin{tabular}{@{}c|c|c|c@{}}
% \toprule
% Instance & Uncoordinated Result & Coordinated Result & $\Delta$ \\ \midrule
% 1 & 2658.5 & 2501.6 & 5.90 \\
% 2 & 2753.5 & 2729.3 & 0.88 \\
% 3 & 2578.5 & 2422.4 & 6.05 \\
% 4 & 4274.6 & 3510.5 & 17.88 \\
% 5 & 4716.6 & 4716.6 & 0.00 \\
% 6 & 5599.9 & 4899.5 & 12.51 \\
% 7 & 5645.1 & 5467.5 & 3.15 \\
% 8 & 5466.7 & 5353.1 & 2.08 \\
% 9 & 2792.7 & 2609.8 & 6.55 \\
% 10 & 2681.7 & 2663.7 & 0.67 \\
% 11 & 2525.6 & 2371.5 & 6.10 \\
% 12 & 4193.3 & 3300.9 & 21.28 \\
% 13 & 4802.6 & 4794.0 & 0.18 \\
% 14 & 5772.1 & 4971.9 & 13.86 \\
% 15 & 5731.7 & 5466.8 & 4.62 \\
% 16 & 5925.7 & 5495.8 & 7.25 \\
% 17 & 2271.3 & 2181.6 & 3.95 \\
% 18 & 3029.1 & 2743.3 & 9.44 \\
% 19 & 3447.8 & 3113.4 & 9.70 \\
% 20 & 3370.9 & 3126.5 & 7.25 \\
% 21 & 5464.3 & 5318.5 & 2.67 \\
% 22 & 4846.3 & 4846.3 & 0.00 \\
% 23 & 6069.2 & 5609.9 & 7.57 \\
% 24 & 6344.6 & 6106.8 & 3.75 \\
% 25 & 2249.6 & 2167.2 & 3.66 \\
% 26 & 3027.4 & 2726.2 & 9.95 \\
% 27 & 3313.4 & 3081.1 & 7.01 \\
% 28 & 3334.1 & 3169.8 & 4.93 \\
% 29 & 5648.3 & 5415.2 & 4.13 \\
% 30 & 4838.9 & 4838.9 & 0.00 \\
% 31 & 5707.3 & 5592.4 & 2.01 \\
% 32 & 5918.8 & 5886.5 & 0.55 \\ \bottomrule
% \end{tabular}
%     \caption{Comparison of total completion times under coordinated and uncoordinated planning.}
%     \label{coord_vs_uncoord}%
% \end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results: Node Prioritization}

In \S 3.1.3, we discussed that network components may have different weights, e.g., representing the number of people that can be served once a component starts functioning. When such weights are taken into account, the order of repairs may change. In this section, we first use the illustrative network in \S 3.1.4 to illustrate the effect of node importance weights on routing decisions. We then repeat our computational experiments with the large sized instances after incorporating node importance weights in order to test the solution performance of the proposed heuristics in solving this extension.

We revise the objective function of the IIRRP model as minimizing the sum of $w_{i} \cross c_{i}, \hspace{0.1em} \forall i \in \mathcal{N}$, where the importance weight of component $i$ is denoted by $w_{i}$. Importance weights, $w_{i}$, are assigned in line with \cite{iloglu2018integrated}, and set at values between zero and one; larger values indicate higher importance. In Table \ref{new_obj_example}, we present three cases with different importance wights and provide the optimal routes attained by the revised IIRRP model. Case I represents our main approach so far, which assumes all components have an equal importance weight of 1. In Case II, weights are set at different values smaller than 1. In Case III, weights are (compared to Case II) decreased for $p_6$ and increased for $p_1$ and $g_1$. As shown in Table \ref{new_obj_example}, nodes $p_1$, $p_2$, $g_1$ and $g_2$ are visited lastly in Case I, but are visited earlier in Case II when their weights are larger compared to the other nodes. Further in Case III, the nodes with the highest importance weights, $p_1$ and $g_1$, are visited at first. On the other hand, in Case III, $p_6$ with the lowest importance weight of 0.1 is not visited the last. This is because a large number of nodes are dependent on $p_6$ and completion times of many are lower if $p_6$ is repaired earlier.

%In computational results so far, we have assumed equal weights for each node. However, including such weights could change the solution of To analyze the effects of such weights, we first gi consider three cases


%In the study, we have approached to problem assuming all infrastructure network components have the equal importance so far yet we bring a new perspective to network structure in this section. We consider consider that it is possible for components to own importance weights in order to function based on a prioritization. The importance weights of components are denoted by $w_{i}$, which is mentioned earlier in \S 3.1.3. Then, we reconstruct the objective function of the IIRRP as minimizing the sum of $w_{i} \cross c_{i}, \hspace{0.1em} \forall i \in N$.  As a consequence, algorithm focuses on not only the latest time that the affected people would get service but also pays attention to the importance weights among the components. 

%In Table \ref{new_obj_example}, we present three case scenarios with different assignment of importance weights among, which all provide the optimal routes for IIRRP. We use the same illustrative network used in \S 3.1.4 in order to indicate the effect of node prioritization more precisely. The Case I represents our main approach, which is based on assuming all components in network have the equal importance, which is 1. In Case II and III, the importance weights differ from node to node. Here, we notice that $p_1$, $p_2$, $g_1$ and $g_2$ nodes that are visited lastly in Case I have been brought forward and visited medially in Case II due to their greater importance weights. Similarly in Case III, the nodes with the highest importance weights, $p_1$ and $g_1$, are visited at first despite being the lastly visited nodes in Case I. Consequently, if there is a matter of urgency regarding specific nodes, they can have different importance weights and we can obtain different routes due to prioritization while also considering the extend of interdependencies nodes have. In addition to all, $p_6$ with 0.1 importance weight in Case III is not visited lastly in the resulted route. The reason is that $p_6$ have a considerable level of interdependencies, which also means it has effect on a great number of nodes and therefore highly affects the routing process. Therefore, it shows that the alternative approach do not only consider the importance weight but also it takes extend of interdependencies into account. We can say that the problem has evolved to focus the composition of both node prioritization based on importance weights and interdependencies of the nodes. 

\begin{table}[H] 
  \centering
  \captionsetup{font=small}
  \footnotesize
\begin{tabular}{crrrrrrrrrrrrc}
    \toprule
    \multirow{2}{*}{\phantom} &  \multicolumn{12}{c}{Node Importance Weights} &  \multirow{2}{*}{Routes}  \\ \cline{2-13}
     &  $p_1$ & $p_2$ & $p_3$ & $p_4$ & $p_5$ & $p_6$ & $g_1$ & $g_2$ & $g_3$ & $g_4$ & $g_5$ & $g_6$ \\ 
    \midrule
    \multirow{2}{*}{Case I}    & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} 
                                                                                                 & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} 
                                                                                                 & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} 
                                                                                                 & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0} & \multirow{2}{*}{1.0}  
                                                                                                 &  $p_0$-$p_6$-$p_5$-$p_4$-$p_2$-$p_1$
              &               & & & & & & & & & & & & &  $g_0$-$g_6$-$g_4$-$g_3$-$g_2$-$g_1$   \\  \midrule
              
    \multirow{2}{*}{Case II}    &  \multirow{2}{*}{0.8} & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.5} 
                                                                                                & \multirow{2}{*}{0.2} & \multirow{2}{*}{0.4} & \multirow{2}{*}{0.4}  
                                                                                                & \multirow{2}{*}{0.8} & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.5} 
                                                                                                & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.2} & \multirow{2}{*}{0.2}
                                                                                                &  $p_0$-$p_6$-$p_2$-$p_1$-$p_5$-$p_4$
             &             & & & & & & & & & & & & & $g_0$-$g_6$-$g_2$-$g_1$-$g_4$-$g_3$    \\  \midrule
             
    \multirow{2}{*}{Case III}  &  \multirow{2}{*}{0.9} & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.5} 
                                                                                                & \multirow{2}{*}{0.2} & \multirow{2}{*}{0.4} & \multirow{2}{*}{0.1}  
                                                                                                & \multirow{2}{*}{0.9} & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.5} 
                                                                                                & \multirow{2}{*}{0.6} & \multirow{2}{*}{0.2} & \multirow{2}{*}{0.2}
                                                                                                &  $p_0$-$p_1$-$p_2$-$p_6$-$p_5$-$p_4$
             &            & & & & & & & & & & & & & $g_0$-$g_1$-$g_2$-$g_6$-$g_4$-$g_3$   \\
    \bottomrule                 
\end{tabular}
\caption{Comparison of optimal routes when node importance weights are taken into account in the illustrative network (\S 3.1.4).}
	\label{new_obj_example}% 
\end{table}

We analyze the performance of our solution algorithms when node importance weights are present. Our constructive heuristic (CH) is revised as follows. Remember that, in the original CH, nodes are prioritized based on their out-degrees, i.e., priority score for each damaged node is selected as the total number of nodes that are directly dependent on it in all infrastructure networks. Now, we take a weighted approach and find the total importance weight of all nodes that are dependent on the damaged node in all networks. In this way, both interdependencies and node importance weights are considered. We also modified $SA_2$ so that the effect of each move is evaluated based on the weighted objective function.

We evaluate $SA_2$ and CH performances with importance weights, which are denoted by $\overline{SA_2}$ and $\overline{CH}$, in set of large instances. Specifically, we modify the instances in Table 4 by randomly assigning importance weights for each node from a continuous uniform $U(0, 1)$ distribution. Table \ref{large_result_new_obj} presents the objective function values, $Z_{\overline{CP}}$, and optimality gaps by solving IIRRP with importance weights by CPLEX. We also provide results obtained by the modified CH and $SA_2$, represented by $\overline{CH}$ and $\overline{SA_2}$. In implementing $\overline{SA_2}$, we solve each instance 10 times, each with a time limit of an hour. As observed from the results, the average performance of $\overline{SA_2}$ is better compared to CPLEX. Moreover, although $\overline{CH}$ gives reasonable initial feasible solutions for the IIRRP in a short time, solutions can be greatly improved by $\overline{SA_2}$. As a result, the main finding of our earlier solution performance analysis remains valid as $\overline{SA_2}$ performs the best when node importance weights are considered.

%In Table \ref{large_result_new_obj}, we evaluate the $SA_2$ performance based on 10 runs and see that $SA_2$ perform better compared to CPLEX for almost all instances. On average, $SA_2$ provides 0.93\% better outputs than CPLEX over all large instances. In addition, it is observed that although CH gives reasonable initial feasible solutions for the IIRRP in a short time, the solutions can be greatly improved by $SA_2$. As a result, the main finding of our analyses remains valid since $SA_2$ performs the best even if we consider the case having importance weights among nodes.

%Moreover, the algorithm behind the constructive heuristic is also adapted to the new perspective of having importance weights. Before, we prioritize repairs of damaged nodes based on their out-degree values, which is considered as a measure to present the criticality of a node. That is to say, we identify a priority score for each damaged node based on the total number of nodes that are directly dependent on it in both infrastructure networks. In this case, we prioritize the repairs for each node based on the total importance weights belonging the nodes that are dependent on regarding node in both networks. In this way, both interdependencies and importance weights are considered as prioritizing repairs for damaged nodes. 
\begin{table}[H]
    \footnotesize
    \centering
\begin{tabular}{ccc ccc ccc cc}
\toprule
% & \multicolumn{2}{c}{$\overline{CPLEX}$} & \multicolumn{2}{c}{$\overline{CH}$} & \multicolumn{3}{c}{$Z_{SA_2}$} & \multicolumn{3}{c}{Gap (\%)} \\ \cmidrule(l){2-11} 
&  &  &  &  & \multicolumn{3}{c}{$Z_{\overline{SA_2}}$} & \multicolumn{3}{c}{Gap (\%)} \\ \cmidrule(l){6-11} 
Inst. & $Z_{\overline{CP}}$ & O.Gap (\%) & $Z_{\overline{CH}}$ & Gap (\%) & min & avg & max & min & avg & max \\ \midrule
\multicolumn{1}{c|}{33} & 6,387.4 & 64.38 & 8,017.4 & 25.22 & 6,333.5 & 6,373.7 & 6,390.0 & -0.84 & -0.21 & 0.04 \\
\multicolumn{1}{c|}{34} & 6,114.1 & 66.79 & 7,717.2 & 26.22 & 6,082.3 & 6,095.8 & 6,110.5 & -0.52 & -0.30 & -0.07 \\
\multicolumn{1}{c|}{35} & 7,248.6 & 67.22 & 9,691.8 & 35.32 & 7,047.7 & 7,107.0 & 7,159.8 & -2.77 & -1.95 & -1.22 \\
\multicolumn{1}{c|}{36} & 7,013.1 & 75.46 & 9,971.6 & 38.20 & 6,747.4 & 6,775.4 & 6,808.1 & -3.79 & -3.39 & -2.92 \\
\multicolumn{1}{c|}{37} & 10,135.7 & 74.97 & 13,676.1 & 34.93 & 10,042.0 & 10,063.3 & 10,111.5 & -0.92 & -0.71 & -0.24 \\
\multicolumn{1}{c|}{38} & 8,483.2 & 73.33 &  11,284.1 & 33.02 & 8,381.7 & 8,383.3 & 8,384.6 & -1.20 & -1.18 & -1.16 \\ 
\multicolumn{1}{c|}{39} & 4,522.0 & 54.66 & 5,660.7 & 25.18 & 4,501.3 & 4,510.0 & 4,519.4 & -0.46 & -0.26 & -0.06 \\
\multicolumn{1}{c|}{40} & 4,418.9 & 54.29 & 5,257.8 & 18.99 & 4,389.4 & 4,401.4 & 4,416.1 & -0.67 & -0.40 & -0.06 \\
\multicolumn{1}{c|}{41} & 4,904.0 & 52.44 & 7,150.3 & 45.81 & 4,875.7 & 4,894.0 & 4,901.5 & -0.58 & -0.20 & -0.05 \\
\multicolumn{1}{c|}{42} & 4,789.9 & 63.85 & 6,892.1 & 43.89 & 4,697.5 & 4,717.8 & 4,779.1 & -1.93 & -1.51 & -0.23 \\
\multicolumn{1}{c|}{43} & 7,190.7 & 63.73 & 9,174.2 & 27.58 & 7,078.3 & 7,140.4 & 7,181.8 & -1.56 & -0.70 & -0.12 \\
\multicolumn{1}{c|}{44} & 5,943.9 & 61.23 & 7,443.6 & 25.23 & 5,902.5 & 5,920.5 & 5,943.9 & -0.70 & -0.39 & 0.00 \\ \hline\hline
\multicolumn{1}{c|}{Avg} &  &  64.36  &  &  31.66  &  &  &  & -1.33 & -0.93 & -0.51 \\ \bottomrule
%\multicolumn{1}{c|}{Avg} & 14,133.3 & 69.16 & 18,432.7 & 30.30 & 13,818.0 & 13,897.5 & 13,965.2 & -2.08 & -1.54 & -1.08 \\ \bottomrule
\end{tabular}
\caption{Solutions of large instances with node importance weights.}
	\label{large_result_new_obj}% 
\end{table}




%Lastly, we evaluate the performances of $SA_2$ and CH in solving larger instances (given in Table \ref{_largeinstance_information}). Table \ref{large_result} presents the solutions obtained by CPLEX within one hour time limit and the corresponding optimality gaps, the CH solution value and its percentage gap with respect to CPLEX solution, and the minimum, average and maximum solution values of $SA_2$ out of 10 runs and their comparison with CPLEX solutions in terms of percentage gaps. According to the results, almost all runs of $SA_2$ perform better than CPLEX for all instances. Note that only in one instance (Instance 43), $SA_2$ runs lead to a solution value (in the worst run) slightly larger than that of CPLEX. On average, $SA_2$ performs 1.54\% better than CPLEX over all large instances. We also observe that although CH provides reasonable initial feasible solutions for the IIRRP very quickly, the solutions can be significantly improved by $SA_2$. Overall, our analyses indicate that a single run of $SA_2$ could be sufficient to support repair planning efforts.





\section{Conclusion} \label{s:conclusion}

In this study, we focus on planning post-disaster infrastructure repair operations by considering interdependencies among different infrastructure networks. Since the functionality of a network component may affect the functionality of other components in the same or different networks, it is important to prioritize repairing critical damaged network components whose functionality may have significant effects on recovery times. We define a new post-disaster network recovery problem, which addresses coordinated routing of repair teams that visit the sites with damaged network components in multiple infrastructures to minimize total recovery times. We present a mathematical model and a practical simulated annealing heuristic to determine the order of repairs for each team. To test the proposed methods, we present a computational study by focusing on a setting with two interdependent infrastructure networks, such as power and gas. 


{\color{blue} This study contributes to the literature in three main ways. First, we introduce a new coordinated post-disaster repair routing problem that incorporates functional interdependencies among multiple lifeline infrastructures. We show that ignoring such interdependencies can cause significant delays in infrastructure recovery times, which may affect survival and well-being of affected people. Secondly, we develop a novel mathematical model, which supports effective prioritization of repairing damaged network components by simultaneously considering infrastructure interdependencies, network travel times, component repair times, as well as the importance of components, which may represent the number of people that can be served once a node starts functioning or the criticality of services provided (e.g., providing electricity to the hospitals). Finally, given the limitations of the mathematical model to solve large size problem instances, we present an efficient simulated annealing metaheuristic that can find high quality solutions for realistic size instances in reasonable times. We design and test intensification and diversification strategies to improve the solution performance of the proposed algorithm.  Our computational results suggest that our metaheuristic, which incorporates an optimization model to address complexities brought by the interdependency structure in our problem, can solve a variety of instances with different characteristics effectively and support making coordinated repair planning after a disaster. In summary, our study makes a theoretical contribution to both disaster management and synchronized vehicle routing literature by defining a new coordinated routing problem that addresses interdependencies among infrastructure networks and a methodological contribution by formulating and solving a difficult optimization problem effectively. Our findings can also facilitate the coordination of post-disaster infrastructure repair efforts in practice.}

Future research can focus on developing alternative solution methods for the proposed coordinated repair routing problem. Moreover, while the proposed problem assumes identical repair teams, in reality, the size and capabilities of repair teams may be different. Future research can extend the proposed problem by incorporating decisions related to developing repair plans in a setting where repair times of damaged components are affected by the size and capabilities of the repair teams.

%Our computational results focus on a setting with two interdependent infrastructure networks, such as power and gas, and indicate that the proposed SA algorithm can find high quality solutions in reasonable times. Our numerical analysis also show that coordinating the repairs of interdependent infrastructures can result in considerable benefits. We consider node importance weights that may represent the number of people that can be served once a node starts functioning, and show that our heuristic performs better in this situation as well. 


%Unlike classical synchronized vehicle routing problems, it does not enforce hard constraints on precedence but affects the objective function value. 

%We develop a mixed integer programming model for the proposed interdependent infrastructure repair scheduling problem, and a heuristic approach to solve realistic sizes of this problem. We test the performance of the two variants of the proposed heuristic. We observe that the equally weighted variant of inter- and intra-network dependencies perform slightly better before any local search methods are applied, but both present similar results after the improvement phase. We could not detect any specific network structure or problem aspect, which could lead one variant to perform better than the other. Therefore, since running times for each version is quite small, we can suggest using both variants to solve a specific instance and apply the best solution obtained. Our modeling and heuristic approach are flexible in that they could be easily extended to cases in which more than two interdependent infrastructures need to be restored in a coordinated way. Another natural extension of this study would be considering a multi-stage dependency structure that allows for multiple levels of cascading effects. 

  %silmeyeyimdiye: The set of time limit of five hours in CPLEX is quite adequate when we consider the problem itself requires an instant remedy due to its post-disaster environment. More precisely, such problems having post-disaster environments are in urgent need of a more time-efficient strategy which requires simplified tools and techniques and also offers better results. However, in an adequate time frame, CPLEX cannot give the optimal solution in complex circumstances of having 12 damaged nodes for each infrastructure network.

%Our study is motivated by the need for effective response of interdependent networks. After a natural disaster, each network may carry out restoration by considering only its own system and then may have to wait for other dependent nodes to be functional. However, when we consider this overall state, neglecting interdependency may cause time and efficiency problems. For these reasons, our study suggests planning by taking into account the interdependencies between networks in the restoration processes. 

%In this study, main focus is on the improvement of interdependent infrastructures that are damaged after disasters. The aim is to restore damaged nodes as soon as possible and enable the whole system back to a functioning state. Minimizing total completion time is the aim of this problem, which represents the time that it takes for all nodes in networks to be functional again. Three different assumptions are implemented in calculations for completion times of nodes in infrastructures: if a node is not damaged but dependent on another node, completion time of such node is the summation of arrival and repair time of the node that is depended on. If a node is damaged but not dependent completion time is the summation of arrival time of repair vehicles and the repair time. Finally, if a node is damaged and interdependent on another node, completion time is the maximum value of summation of own arrival and repair time or the summation of the arrival and repair time of the node that is dependent on. A mathematical model is developed with respect to the interdependency and the functionality of the infrastructures, but it is challenging and complex. After development of the mathematical model two constructive greedy heuristics: peer and leader-follower and two different versions of these heuristics: selfless and selfish versions are developed. Initial solutions from constructive heuristics are improved using local search improvement methods: swap, exchange and remove and insert methods. In order to test these constructive heuristics, different types of instances are created and represented. Finally, constructive and improvement heuristics solutions are compared with the solutions obtained from CPLEX.

%The type and time of constructive heuristic to be used depend on the structure of nodes in the disaster area. As the number of damaged nodes and dependency between networks increase, total completion times increase due to prolonged repair times. Although there is no difference in selfless and selfish version instances with 20 nodes, selfish version performs better in instances with 50 and 100 nodes. All of the solutions from constructive heuristics are improved using the iterative implementation of neighborhood search algorithms. For instances with 20 nodes optimal solution is obtained. For instances with 50 and 100 nodes, we manage to obtain better results than CPLEX results, which are considered as benchmark solutions with optimality gaps. The worst performance among the results from local search improvement methods, actually performs $\%$ 1.9 better than the solutions that are obtained from CPLEX. On the other hand, the most improved instance performs $\%$ 23.69 better when compared to the solutions from CPLEX. Thanks to this better performance, we observe that local search improvement methods are very important in the improvement of these constructive heuristics. The run times of both constructive heuristics and improvement heuristics are three minutes on average. Relatively short run times of these heuristics make them very advantageous in the event of disasters, when each second matters in the process of decision making.  

%Given the fact that restoration of interdependent infrastructures after disasters is a developing research area, biggest contribution of this work is a routing model with bidirectional interdependencies. In the previous studies in the literature, even if interdependency is considered, it has been only studied as one directional interdependency, examining the effects of just one network on another. In addition, approaching this problem with our heuristic, which is accessible and executable in real life, may contribute to disaster coordination centers to a large extent in means of managerial insights. Testing this model in uncertain post disaster scenarios may be possible future research topic, assuming that restoration subject is a new and developing research area.

%\footnote{BITTI! INANAMIYORUM :)}

\bibliographystyle{chicago}
%\bibliographystyle{apa}


\bibliography{references}

\newpage

% \section*{Appendix A. Algorithm Steps} \label{s:appE}
% The main steps of our heuristics are outlined below.

% \renewcommand{\baselinestretch}{1.4}

% \begin{table}[H]
% %  \centering
%   \scriptsize
% \begin{tabular} {l l }
% \hline
% \multicolumn{2}{l}{\textbf{Steps of the SA}} \\
% \hline
% \hline
% \multicolumn{2}{l}{\textbf{Step 0. Initialize algorithm parameters.}} \\
% \hline
% & Set: cooling parameter ($\alpha$), initial temperature ($T_{0}$), epoch length ($L$), intensification starting iteration ($I_k$), \\
% & termination criteria (solution time limit, final temperature ($T_{f}$)). \\
% & $k$: iteration index; $z*$: best total recovery time; $z_{k}$: current total recovery time. \\
% & Set: $k=0$. \\
% \hline
% \multicolumn{2}{l}{\textbf{Step 1. Generate an initial solution: Constructive Heuristic (CH)}} \\
% &  \textbf{1a.} Calculate the out-degree scores of damaged power $(B_p)$ and gas $(B_g)$ nodes, and sort them in descending order. \\
% \multicolumn{2}{l}{\textit{\textbf{WHILE (there is an unrouted node) DO \{}}}\\
% &  \textbf{1b.} Assign the node with the highest score to a vehicle. \\
% &  \textbf{1c.} Append the next node with the highest score to an existing route (by evaluating a travel time threshold ($\Gamma$). \\ 
% & If no existing route satisfies the $\Gamma$ threshold, assign the current node to an idle vehicle and initiate a new route.\\
% &  \textbf{1d.} If multiple routes meet the $\Gamma$ threshold, assign the node to the vehicle that leads to the minimum increase \\
% & in the maximum recovery time (by solving (2)-(15)). \\
% &  \textbf{1e.} If all vehicles have at least one node assigned, pick the remaining nodes sequentially, and append to the route \\
% & that leads to the minimum increase in the maximum recovery time (by solving (2)-(15)). \\
% &  \textbf{1f.} Evaluate the node recovery times, $c_{i}$, and objective function of the solution by solving (3)-(17) once the \\
% &  routes are constructed. \\
% \multicolumn{2}{l}{\textbf{\}}}\\
% \\ 
 
% \multicolumn{2}{l}{Set current and the best solutions: Output of the CH} \\ 
 
% \hline
%  \multicolumn{2}{l}{\textit{\textbf{WHILE (a termination criterion is not met) DO \{}}}\\
%  \multicolumn{2}{l}{\textit{\textbf{FOR (i=1 to $L$) DO \{}}}\\
%  \multicolumn{2}{l}{\textbf{Step 2. Generate and evaluate neighbor solutions}:}\\
% & \textbf{i.} Whether the moves are first applied to the power or gas network is randomly selected. \\
% & \textbf{ii.} Each neighborhood move is applied consecutively to both networks in the order specified. \\

% & \textit{\textbf{Step 2a. Apply 2-Opt move}}. \\
% & \textbf{i.} Remove two links in a given route and create new links to reconnect the nodes in a different way. All pairs \\ 
% & of links are considered in implementing the move. \\
% & \textbf{ii.} For each pair, break links $(i_1, i_2)$ and $(j_1, j_2)$ and make links $(i_1, j_1)$ and $(i_2, j_2)$. Apply \textbf{\textit{Step 4a}}. \\

% & \textit{\textbf{Step 2b. Apply Swap move}}.\\
% & \textbf{i.}  The move is applied to all routes within the network. All possible combinations of sequences are examined \\ 
% & by changing the positions of each pair of nodes in all routes. \\
% & \textbf{ii.} Consider all pair of nodes in the same route $(i_1, i_2)$, A new solution is obtained by replacing the $i_1$ and $i_2$ \\
% &  nodes. Apply \textbf{\textit{Step 4a}}. \\

% &  \textit{\textbf{Step 2c. Apply Exchange move}}.\\
% & \textbf{i.} This move exchanges nodes between two different routes. Specifically, each node in a route is exchanged  \\
% & with another node in another route. This is repeated until all possible node exchanges between all pairs of  \\
% & routes are evaluated. \\
% & \textbf{ii.} Remove nodes $i$ and $j$ from routes $x$ and $y$, respectively; and insert nodes $i$ and $j$ to routes $y$ and $x$ to \\
% &  the same location where its pair is removed from, respectively. Apply \textbf{\textit{Step 4a}}.\\

% &  \textit{\textbf{Step 2d. Apply Remove \& Insert move}}.\\
% & \textbf{i.} A node remove from its route and insert into a different route, all possible solutions are evaluated. \\
% & \textbf{ii.} Remove nodes $i$ from routes $x$; and insert nodes $i$ to routes $y$. Apply \textbf{\textit{Step 4a}}.\\


% &  \textit{\textbf{Step 2e. Evaluate a candidate solution obtained by a neighborhood  moves (Metropolis Criterion)}}.\\
% & \textbf{i.} Check whether a candidate solution is admissible; Calculate the $\Delta E = z_{k} - z*$. \\
% & \textbf{ii. } If $\Delta E > 0$, accept the current solution and update the best solution. Apply \textbf{\textit{Step 4b}}. \\
% & \textbf{iii. } Else, generate random number $r, 0 \leq r \leq 1$. If $e^{-\Delta/T} > 0$, accept $z_{k}$ and update $z*$.\\
% & \textbf{iv. } If there is no admissible solution, reject the $z_{k}$ and apply the next neighborhood move.  \\
% \multicolumn{2}{l}{\textbf{\}}}\\
% \hline
% \multicolumn{2}{l}{\textbf{Step 3. Update.}}\\
% & $k:=k+1$; $T_{k+1} = \alpha \times T_{k}$ \\
% \multicolumn{2}{l}{\textbf{Step 4. Apply Intensification/Diversification}:}\\
% &   \textit{\textbf{Step 4a. Diversification}}.\\
% & \textbf{i.} The number of $N$ best solutions resulted from each move is recorded. If the best solution among $N$  \\
% & solutions is better than $z*$, the best solution among $N$ solutions is accepted as the new best solution as \\
% & a result of the move. \\
% & \textbf{ii.} If not, a random solution among $N$ solutions is selected and accepted as the result of the move.  \\
% &   \textit{\textbf{Step 4b. Intensification}}. \\
% & \textbf{i.} This strategy is applied after the current iteration $k$ is greater than $I_k$. \\
% & \textbf{ii.} If the incumbent is updated ($z_{k} > z*$) , the number of epochs in this iteration is temporarily increased so \\
% & that the search deepens at a fixed temperature (Update  $L$). \\
% & \textbf{iii.} At the end of the iteration in which the strategy is applied, L returns to its initial value in the algorithm. \\
% \multicolumn{2}{l}{\textbf{\}}}\\
% \hline
% \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table}[!htp]
%   \centering
%   \footnotesize
% \begin{tabular} {l l}
% \hline
% \multicolumn{2}{l}{\textbf{Steps of the CH}} \\
% \hline
% &  \textbf{1a.} Calculate the out-degree scores of Broken Power $(B_p)$ and Gas $(B_g)$ Nodes. Sort them in descending  
% \\
% & order. \\
% &  \textbf{1b.} Assign the node with the highest priority to a vehicle. \\
% &  \textbf{1c.} Assign the next node with the highest out-degree score to a vehicle by evaluating a travel time threshold \\
% & prespecified for each infrastructure network, denoted by $\Gamma$. \\
% &  \textbf{1d.} Monitor the travel time ($t_{ij}$) between the current node and the current positions of the vehicles which are \\
% & already assigned a node.  
% \\
% & \textbf{1e.} If $t_{ij}$ between the current node and the vehicle’s last visited node is less than the corresponding $\Gamma$, \\
% & append the current node to the path of the vehicle. 
% \\
% &  \textbf{1f.} If multiple such vehicles, assign the node to the vehicle that leads to the minimum increase in the \\
% & maximum path duration after its addition to the current path. 
% \\
% &  \textbf{1g.} If no vehicle satisfying the corresponding $\Gamma$, assign the current node to an idle vehicle. Initiate a new \\
% & route. \\
% &  \textbf{1h.} Repeat the previous step until all vehicles have at least one node assigned.
% \\ 
% &  \textbf{1i.} Pick the remaining sorted nodes form the priority list sequentially, if any. Assign the nodes to the vehicle \\
% & that leads to the minimum increase in the maximum path duration. 
% \\ 
% &  \textbf{1j.} Evaluate the node recovery times, $c_{i}$, and objective function of the solution by solving (3)-(17) once the \\
% &  routes are constructed.
% \\ 
 
% \multicolumn{2}{l}{Set current and the final solutions: Output of the CH} \\
% % & $z*$: best minimum coverage ratio; $r*$: best total route duration.  \\
% % & $z_{t}$: current minimum coverage ratio; $r_{t}$: current total route duration.  \\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}[]
%   \centering
%   \footnotesize
% \begin{tabular} {l l }
% \hline
% \multicolumn{2}{l}{\textbf{Steps of the SA}} \\
% \hline
% \hline
% \multicolumn{2}{l}{\textbf{Step 0. Initialization:}} \\
% \hline
% &  \textit{Set and initialize algorithm parameters}: lists for all neighborhood moves, cooling parameter ($\alpha$), \\
% & termination criteria; beginning temperature ($T_{0}$), final temperature ($T_{f}$), epoch length ($L$),\\
% &  Intensification starting iteration ($I_k$). \\
% &  $k$ : iteration index; Set $k=0$. \\
% \hline
% \multicolumn{2}{l}{\textbf{Step 1. Generate an initial solution by applying CH}} \\
%  \multicolumn{2}{l}{Set current and the best solutions: Output of the CH} \\
%  & $z*$: best total recovery time; $z_{k}$: current total recovery time. \\
% \hline
%  \multicolumn{2}{l}{\textit{\textbf{WHILE (a termination criterion is not met) DO \{}}}\\
%  \multicolumn{2}{l}{\textit{\textbf{FOR i=1 to $L$ DO \{}}}\\
%  \multicolumn{2}{l}{\textbf{Step 2. Generate and evaluate neighbor solutions}:}\\
% & \textbf{i.} Whether the moves are first applied to the power or gas network is randomly selected. \\
% & \textbf{ii.} Each neighborhood move is applied consecutively to both networks in the order specified. \\

% & \textit{\textbf{Step 2a. Apply 2-Opt move}}. \\
% & \textbf{i.} Remove two links in a given route and create new links to reconnect the nodes in a different way. All pairs \\ 
% & of links are considered in implementing the move. \\
% & \textbf{ii.} For each pair, break links $(i_1, i_2)$ and $(j_1, j_2)$ and make links $(i_1, j_1)$ and $(i_2, j_2)$. Apply \textbf{\textit{Step 4a}}. \\

% & \textit{\textbf{Step 2b. Apply Swap move}}.\\
% & \textbf{i.}  The move is applied to all routes within the network. All possible combinations of sequences are examined \\ 
% & by changing the positions of each pair of nodes in all routes. \\
% & \textbf{ii.} Consider all pair of nodes in the same route $(i_1, i_2)$, A new solution is obtained by replacing the $i_1$ and $i_2$ \\
% &  nodes. Apply \textbf{\textit{Step 4a}}. \\

% &  \textit{\textbf{Step 2c. Apply Exchange move}}.\\
% & \textbf{i.} This move exchanges nodes between two different routes. Specifically, each node in a route is exchanged  \\
% & with another node in another route. This is repeated until all possible node exchanges between all pairs of  \\
% & routes are evaluated. \\
% & \textbf{ii.} Remove nodes $i$ and $j$ from routes $x$ and $y$, respectively; and insert nodes $i$ and $j$ to routes $y$ and $x$ to \\
% &  the same location where its pair is removed from, respectively. Apply \textbf{\textit{Step 4a}}.\\

% &  \textit{\textbf{Step 2d. Apply Remove \& Insert move}}.\\
% & \textbf{i.} A node remove from its route and insert into a different route, all possible solutions are evaluated. \\
% & \textbf{ii.} Remove nodes $i$ from routes $x$; and insert nodes $i$ to routes $y$. Apply \textbf{\textit{Step 4a}}.\\


% &  \textit{\textbf{Step 2e. Evaluate a candidate solution obtained by a neighborhood  moves (Metropolis Criterion)}}.\\
% & \textbf{i.} Check whether a candidate solution is admissible; Calculate the $\Delta E = z_{k} - z*$. \\
% & \textbf{ii. } If $\Delta E > 0$, accept the current solution and update the best solution. Apply \textbf{\textit{Step 4b}}. \\
% & \textbf{iii. } Else, generate random number $r, 0 \leq r \leq 1$. If $e^{-\Delta/T} > 0$, accept $z_{k}$ and update $z*$.\\
% & \textbf{iv. } If there is no admissible solution, reject the $z_{k}$ and apply the next neighborhood move.  \\
% \multicolumn{2}{l}{\textbf{\}}}\\
% \hline
% \multicolumn{2}{l}{\textbf{Step 3. Update.}}\\
% & $k:=k+1$; $T_{k} = \alpha \times T_{k-1}$ \\
% \multicolumn{2}{l}{\textbf{Step 4. Apply Intensification/Diversification}:}\\
% &   \textit{\textbf{Step 4a. Diversification}}.\\
% & \textbf{i.} The number of $N$ best solutions resulted from each move is recorded. If the best solution among $N$  \\
% & solutions is better than $z*$, the best solution among $N$ solutions is accepted as the new best solution as \\
% & a result of the move. \\
% & \textbf{ii.} If not, a random solution among $N$ solutions is selected and accepted as the result of the move.  \\
% &   \textit{\textbf{Step 4b. Intensification}}. \\
% & \textbf{i.} This strategy is applied after the current iteration $k$ is greater than $I_k$. \\
% & \textbf{ii.} If the incumbent is updated ($z_{k} > z*$) , the number of epochs in this iteration is temporarily increased so \\
% & that the search deepens at a fixed temperature (Update  $L$). \\
% & \textbf{iii.} At the end of the iteration in which the strategy is applied, L returns to its initial value in the algorithm. \\
% \multicolumn{2}{l}{\textbf{\}}}\\
% \hline
% \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\newpage
\section*{Appendix A. Interdependency parameters} \label{s:appB}

We provide the interdependency parameters for the illustrative example presented in \S 3.1.4 below.

\begin{table}[H]
  \centering
  \small
  %\captionsetup{font=footnotesize}
\begin{tabular}{@{}ccccccccccccccccc@{}}
\toprule
\multicolumn{17}{c}{Interdependency Structure}                                                                                    \\ \midrule
\multicolumn{8}{c}{Power-Power}                                         &  & \multicolumn{8}{c}{Gas-Gas}                                              \\ \cmidrule(r){1-8} \cmidrule(l){10-17} 
                \diagbox[innerleftsep=.5cm,innerrightsep=0pt,width=2em,trim=l,height=1.5\line]{$i$}{$j$} &$p_0$ & $p_1$ & $p_2$ & $p_3$ & $p_4$ & $p_5$ & $p_6$   &  &  \diagbox[innerleftsep=.5cm,innerrightsep=0pt,width=2em,trim=l,height=1.5\line]{$i$}{$j$}& $g_0$ & $g_1$ & $g_2$ & $g_3$ & $g_4$ & $g_5$ & $g_6$  \\ \cmidrule(lr){2-8} \cmidrule(l){11-17} 
\multicolumn{1}{c|}{$p_0$}   & 1    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$g_0$} & 1    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_1$}   & 0    & 1  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{1} &  & \multicolumn{1}{c|}{$g_1$}   & 0    & 1  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_2$}   & 0    & 0  & 1  & 0  & 0  & 0  & \multicolumn{1}{c|}{1} &  & \multicolumn{1}{c|}{$g_2$}   & 0    & 0  & 1  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_3$}   & 0    & 0  & 0  & 1  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$g_3$}   & 0    & 0  & 0  & 1  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_4$}   & 0    & 0  & 0  & 0  & 1  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$g_4$}   & 0    & 0  & 0  & 1  & 1  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_5$}   & 0    & 0  & 0  & 0  & 0  & 1  & \multicolumn{1}{c|}{1} &  & \multicolumn{1}{c|}{$g_5$}   & 0    & 0  & 0  & 0  & 1  & 1  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$p_6$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{1} &  & \multicolumn{1}{c|}{$g_6$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{1}  \\ \cmidrule(r){1-8} \cmidrule(l){10-17} 
                          &      &    &    &    &    &    &                        &  &                           &      &    &    &    &    &    &                         \\
\multicolumn{8}{c}{Power-Gas}                                           &  & \multicolumn{8}{c}{Gas-Power}                                            \\ \cmidrule(r){1-8} \cmidrule(l){10-17} 
            \diagbox[innerleftsep=.5cm,innerrightsep=0pt,width=2em,trim=l,height=1.5\line]{$i$}{$j$} &$p_0$ & $p_1$ & $p_2$ & $p_3$ & $p_4$ & $p_5$ & $p_6$    &  &     \diagbox[innerleftsep=.5cm,innerrightsep=0pt,width=2em,trim=l,height=1.5\line]{$i$}{$j$}& $g_0$ & $g_1$ & $g_2$ & $g_3$ & $g_4$ & $g_5$ & $g_6$ \\ \cmidrule(lr){2-8} \cmidrule(l){11-17} 
\multicolumn{1}{c|}{$g_0$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_0$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_1$}   & 0    & 1  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_1$}   & 0    & 0  & 1  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_2$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_2$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_3$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_3$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_4$}   & 0    & 0  & 0  & 0  & 0  & 1  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_4$}   & 0    & 0  & 0  & 1  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_5$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_5$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0}  \\
\multicolumn{1}{c|}{$g_6$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{0} &  & \multicolumn{1}{c|}{$p_6$}   & 0    & 0  & 0  & 0  & 0  & 0  & \multicolumn{1}{c|}{1} \\
\cmidrule(r){1-8} \cmidrule(l){10-17} 
\bottomrule    
\end{tabular}
\caption{Interdependency parameters ($d_{ij}$ values) for the network in the illustrative example.}
	\label{illustrative_dependency}% 
\end{table}


\newpage
\section*{Appendix B. The Instance Generator} \label{s:appC}
We briefly introduce our Instance Generator, which is a dashboard developed by using R programming language. This dashboard allows us to generate IIRRP test instances that involve two interdependent infrastructure networks systematically. We provide the source code in R and its output (the dashboard) integrated in a website. 

The dashboard, which is shown in Figure 4, generates the input files for the IIRRP model and the SA heuristic. Additionally, it visualizes the generated infrastructure network as in Figure 2a). The dashboard accepts a single input file (network\_input.txt in the figure), which lists the coordinates of the depots and sites involving the infrastructure nodes in the network. All other parameter levels are set through the dashboard.  Based on the selected parameter levels, the input data are automatically generated as described in \S 5.1.

%These parameters are the total number of nodes and their characteristic distribution (e.g. the number of power/gas nodes and vehicles, damaged number of power/gas nodes). Additionally, the other parameters are related to their repairing process and dependencies to each other. For instance, how much time is needed to repair power and gas nodes and the nodes’ interdependency. There are dependencies among not only with the same type of nodes but also the crosswise. As determining the dependencies, we aim to keep the complexity of the problem alive in order to stay close to the real-life scenario for the problem. The dependencies among the same type of nodes and the crosswise of those are assigned partially random, partially damaged on purpose. The number of those are under control of the user. The reason for that is eliminating the possible selection of nodes which have dependencies but all non-damaged. In such a case, the dependency issue would be redundant, so this circumstance is prevented and kept controlled.


\begin{figure}[http!]
	\centering
	\includegraphics[width=1.0\textwidth, angle=0]{input_generator2.JPG} \hfill
	\caption{Instance generator.}
\end{figure}



\newpage
\section*{Appendix C. Results: Effects of Cooling Parameter} \label{s:appD}
In our computational experiments, we use a geometric cooling schedule and present results obtained with $\alpha=0.90$. We test the performance of the proposed $SA_2$ algorithm by using alternative cooling parameters. Here we present results obtained by $\alpha=0.85$ and $\alpha=0.95$. Table \ref{sa_cooling_effects} shows the percentage gap between solutions obtained by $SA_2$ and CPLEX for 16 small instances (i.e., Set 1 instances), similar to Table 5.

For $\alpha=0.90$, it takes $SA_2$ 1,179 CPU seconds on average to solve these instances, which decreases to 739 when $\alpha=0.85$. However, we observe that solution performance worsens when we cool the temperature at a faster rate. For example,  when $\alpha$ is reduced to 0.85, the average gap percentage with respect to CPLEX increases to 0.16\% from 0.07\%; furthermore, average $n_{best}$ decreases from 5.9 to 4.3. On the contrary, we observe that the performance is improved when $\alpha=0.95$; for example, average gap decreases to $-0.01$. However, we observe a significant increase in average CPU time (i.e., about 70\%) when algorithm is cooled slower. These results show the trade-off between the solution quality and time. Decision makers can choose a proper cooling rate depending on the urgency of planning on the ground. In our experiments, we use $\alpha=0.90$ that balances solution quality and time.







\begin{table}[H]
    \centering
    %\tiny
    \scriptsize
\begin{tabular}{ccccccccccccccc}
\toprule
 & \multicolumn{7}{c}{$\alpha=0.85$} & \multicolumn{7}{c}{$\alpha=0.95$} \\ \cline{3-7} \cline{10-14}
 & \multicolumn{3}{c}{Gap (\%)} &  & \multicolumn{3}{c}{CPU time (s)} & \multicolumn{3}{c}{Gap (\%)} &  & \multicolumn{3}{c}{CPU time (s)} \\ \cline{2-4} \cline{6-8}  \cline{9-11} \cline{13-15} 
Inst. & min & max & avg & $n_{best}$ & min & max & avg & min & max & avg & $n_{best}$ & min & max & avg \\ \hline
\multicolumn{1}{c|}{1} & -0.98 & -0.17 & \multicolumn{1}{c|}{-0.74} & \multicolumn{1}{c|}{2} & 927 & 1,119 & \multicolumn{1}{c|}{1,057} & -0.98 & -0.17 & \multicolumn{1}{c|}{-0.84} & \multicolumn{1}{c|}{6} & 2,732 & 3,446 & 2,960 \\
\multicolumn{1}{c|}{2} & 0.41 & 2.68 & \multicolumn{1}{c|}{1.17} & \multicolumn{1}{c|}{0} & 916 & 1,162 & \multicolumn{1}{c|}{1,083} & 0.00 & 0.57 & \multicolumn{1}{c|}{0.43} & \multicolumn{1}{c|}{1} & 2,850 & 3,552 & 3,038 \\
\multicolumn{1}{c|}{3} & 0.07 & 0.62 & \multicolumn{1}{c|}{0.29} & \multicolumn{1}{c|}{0} & 951 & 1,145 & \multicolumn{1}{c|}{1,082} & 0.00 & 0.29 & \multicolumn{1}{c|}{0.17} & \multicolumn{1}{c|}{1} & 2,810 & 3,453 & 3,013 \\
\multicolumn{1}{c|}{4} & -0.31 & 0.47 & \multicolumn{1}{c|}{0.05} & \multicolumn{1}{c|}{1} & 916 & 1,167 & \multicolumn{1}{c|}{1,087} & -0.31 & 0.00 & \multicolumn{1}{c|}{-0.20} & \multicolumn{1}{c|}{3} & 2,886 & 3,551 & 3,044 \\ \hline
\multicolumn{1}{c|}{5} & 0.00 & 0.56 & \multicolumn{1}{c|}{0.16} & \multicolumn{1}{c|}{6} & 356 & 437 & \multicolumn{1}{c|}{411} & 0.00 & 0.48 & \multicolumn{1}{c|}{0.05} & \multicolumn{1}{c|}{9} & 1,096 & 1,330 & 1,163 \\
\multicolumn{1}{c|}{6} & 0.00 & 0.20 & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{7} & 342 & 421 & \multicolumn{1}{c|}{399} & 0.00 & 0.08 & \multicolumn{1}{c|}{0.02} & \multicolumn{1}{c|}{7} & 1,042 & 1,263 & 1,120 \\
\multicolumn{1}{c|}{7} & 0.00 & 0.39 & \multicolumn{1}{c|}{0.11} & \multicolumn{1}{c|}{5} & 338 & 434 & \multicolumn{1}{c|}{397} & 0.00 & 0.20 & \multicolumn{1}{c|}{0.07} & \multicolumn{1}{c|}{6} & 1,022 & 1,269 & 1,121 \\
\multicolumn{1}{c|}{8} & 0.00 & 0.87 & \multicolumn{1}{c|}{0.11} & \multicolumn{1}{c|}{6} & 351 & 437 & \multicolumn{1}{c|}{413} & 0.00 & 0.03 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{8} & 1,100 & 1,314 & 1,150 \\ \hline
\multicolumn{1}{c|}{9} & 0.00 & 0.17 & \multicolumn{1}{c|}{0.13} & \multicolumn{1}{c|}{1} & 902 & 1,167 & \multicolumn{1}{c|}{1,053} & 0.00 & 0.17 & \multicolumn{1}{c|}{0.06} & \multicolumn{1}{c|}{5} & 2,736 & 3,341 & 2,887 \\
\multicolumn{1}{c|}{10} & 0.00 & 2.33 & \multicolumn{1}{c|}{0.78} & \multicolumn{1}{c|}{4} & 956 & 1,127 & \multicolumn{1}{c|}{1,076} & 0.00 & 0.54 & \multicolumn{1}{c|}{0.05} & \multicolumn{1}{c|}{9} & 2,870 & 3,403 & 3,054 \\
\multicolumn{1}{c|}{11} & 0.00 & 0.50 & \multicolumn{1}{c|}{0.08} & \multicolumn{1}{c|}{5} & 940 & 1,151 & \multicolumn{1}{c|}{1,075} & 0.00 & 0.11 & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{9} & 2,773 & 2,996 & 2,896 \\
\multicolumn{1}{c|}{12} & 0.00 & 0.53 & \multicolumn{1}{c|}{0.21} & \multicolumn{1}{c|}{2} & 915 & 1,174 & \multicolumn{1}{c|}{1,086} & -0.07 & 0.25 & \multicolumn{1}{c|}{0.06} & \multicolumn{1}{c|}{2} & 2,898 & 3,071 & 2,963 \\ \hline
\multicolumn{1}{c|}{13} & 0.00 & 0.79 & \multicolumn{1}{c|}{0.09} & \multicolumn{1}{c|}{8} & 365 & 440 & \multicolumn{1}{c|}{411} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,105 & 1,230 & 1,149 \\
\multicolumn{1}{c|}{14} & 0.00 & 0.13 & \multicolumn{1}{c|}{0.06} & \multicolumn{1}{c|}{5} & 347 & 422 & \multicolumn{1}{c|}{393} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,065 & 1,143 & 1,100 \\
\multicolumn{1}{c|}{15} & 0.00 & 0.40 & \multicolumn{1}{c|}{0.10} & \multicolumn{1}{c|}{7} & 330 & 438 & \multicolumn{1}{c|}{392} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,043 & 1,141 & 1,096 \\
\multicolumn{1}{c|}{16} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 354 & 437 & \multicolumn{1}{c|}{401} & 0.00 & 0.00 & \multicolumn{1}{c|}{0.00} & \multicolumn{1}{c|}{10} & 1,056 & 1,148 & 1,104 \\\hline \hline

\multicolumn{1}{c|}{Avg} & -0.05 & 0.65 & \multicolumn{1}{c|}{0.16} & \multicolumn{1}{c|}{4.3} & 638 & 792 & \multicolumn{1}{c|}{739} & -0.08 & 0.16 & \multicolumn{1}{c|}{-0.01} & \multicolumn{1}{c|}{6.6} & 1,943 & 2,291 & 2,054 \\ 
 \bottomrule
\end{tabular}
\caption{Comparison of $SA_2$ solutions with different cooling parameters and $CPLEX$ solutions for small instances (Set 1).}
	\label{sa_cooling_effects}% 
\end{table}

\end{document}

